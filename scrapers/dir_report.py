import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DIR (å¤§å’Œç¸½ç ”)...")
    base_url = "https://www.dir.co.jp"
    target_url = "https://www.dir.co.jp/report/research/economics/index.html"
    reports = []
    
    try:
        resp = requests.get(target_url, headers=HEADERS, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        news_list = soup.find('ul', class_='c-newsList')
        if not news_list: return reports
        
        for item in news_list.find_all('li', class_='c-newsList-item'):
            link_tag = item.find('a', class_='c-newsList-link')
            if not link_tag or not link_tag.get('href'): continue
            
            title_tag = link_tag.find('p', class_='title')
            if not title_tag: continue
            title = title_tag.get_text(strip=True)
            
            date_tag = link_tag.find('p', class_='date')
            date_text = date_tag.get_text(strip=True) if date_tag else "æœªçŸ¥æ—¥æœŸ"
            
            if not is_within_30_days(date_text) or len(title) < 5 or any(kw in title for kw in ["ãƒ‡ãƒ¼ã‚¿ãƒ–ãƒƒã‚¯", "é€±æ¬¡"]): continue
            
            pdf_url = link_tag.get('href').replace('.html', '.pdf')
            final_pdf = urljoin(base_url, pdf_url)
            
            reports.append({"Source": "DIR", "Date": date_text, "Name": title, "Link": final_pdf})
        
    except Exception as e:
        print(f"  âŒ DIR å¤±æ•—: {e}")
    
    print(f"  âœ… DIR æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
