import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–çˆ¬èŸ²
    
    ä¾†æº: https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html
    æŠ“å–: æœ€è¿‘ 30 å¤©å…§çš„å¸‚å ´è©•è«– PDF å ±å‘Š
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– CTBC (ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–)...")
    reports = []
    seen_urls = set()
    
    base_url = "https://www.ctbcbank.com"
    target_url = "https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html"
    
    try:
        # å–å¾—åˆ—è¡¨é 
        resp = requests.get(target_url, headers=HEADERS, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ç­–ç•¥ 1: ç›´æ¥æ‰¾ PDF é€£çµ
        pdf_links = soup.find_all('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
        
        if pdf_links:
            print(f"  æ‰¾åˆ° {len(pdf_links)} å€‹ PDF é€£çµ")
            
            for link in pdf_links:
                pdf_url = urljoin(base_url, link['href'])
                
                if pdf_url in seen_urls:
                    continue
                seen_urls.add(pdf_url)
                
                # æå–æ¨™é¡Œ
                title = link.get_text(strip=True)
                
                # å¦‚æœé€£çµæ–‡å­—å¤ªçŸ­,å¾çˆ¶å…ƒç´ æ‰¾
                if len(title) < 5:
                    parent = link.find_parent(['li', 'div', 'tr', 'td'])
                    if parent:
                        title = re.sub(r'\s+', ' ', parent.get_text(strip=True))
                
                # é‚„æ˜¯å¤ªçŸ­å°±ç”¨æª”å
                if len(title) < 5:
                    from urllib.parse import unquote
                    filename = pdf_url.split('/')[-1].replace('.pdf', '')
                    title = unquote(filename)
                
                # æå–æ—¥æœŸ
                date_text = None
                parent = link.find_parent(['li', 'div', 'tr', 'td'])
                
                if parent:
                    search_text = parent.get_text()
                else:
                    search_text = title + " " + pdf_url
                
                # å¤šç¨®æ—¥æœŸæ ¼å¼
                date_patterns = [
                    r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
                    r'20\d{2}/\d{1,2}/\d{1,2}',
                    r'20\d{2}\.\d{1,2}\.\d{1,2}',
                    r'20\d{2}-\d{1,2}-\d{1,2}',
                ]
                
                for pattern in date_patterns:
                    match = re.search(pattern, search_text)
                    if match:
                        date_text = match.group(0)
                        break
                
                # æ²’æ—¥æœŸå°±è·³é
                if not date_text:
                    continue
                
                # æª¢æŸ¥ 30 å¤©å…§
                if not is_within_30_days(date_text):
                    continue
                
                # æ¸…ç†æ¨™é¡Œ
                title = re.sub(r'20\d{2}[å¹´/.\-]\d{1,2}[æœˆ/.\-]\d{1,2}[æ—¥]?', '', title)
                title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
                title = re.sub(r'\s+', ' ', title).strip()
                
                reports.append({
                    "Source": "CTBC",
                    "Date": date_text,
                    "Name": title,
                    "Link": pdf_url
                })
        
        # ç­–ç•¥ 2: å¦‚æœæ²’æœ‰ç›´æ¥ PDF,æ‰¾å ±å‘Šè©³æƒ…é 
        if len(reports) == 0:
            print("  æœªæ‰¾åˆ°ç›´æ¥ PDF,å˜—è©¦å ±å‘Šè©³æƒ…é ...")
            
            report_keywords = ['å ±å‘Š', 'è©•è«–', 'åˆ†æ', 'å±•æœ›', 'è§€é»']
            detail_links = []
            
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text(strip=True)
                
                if '.pdf' not in href.lower() and any(kw in text for kw in report_keywords):
                    full_url = urljoin(base_url, href)
                    if 'ctbcbank.com' in full_url:
                        detail_links.append((full_url, text))
            
            print(f"  æ‰¾åˆ° {len(detail_links)} å€‹è©³æƒ…é ")
            
            # è¨ªå•è©³æƒ…é 
            for detail_url, preview_title in detail_links[:20]:
                if detail_url in seen_urls:
                    continue
                seen_urls.add(detail_url)
                
                try:
                    detail_resp = requests.get(detail_url, headers=HEADERS, timeout=10)
                    detail_resp.encoding = 'utf-8'
                    detail_soup = BeautifulSoup(detail_resp.text, 'html.parser')
                    
                    # æ‰¾ PDF
                    pdf_link = detail_soup.find('a', href=re.compile(r'\.pdf$', re.I))
                    if not pdf_link:
                        continue
                    
                    pdf_url = urljoin(detail_url, pdf_link['href'])
                    if pdf_url in seen_urls:
                        continue
                    seen_urls.add(pdf_url)
                    
                    # æ¨™é¡Œ
                    h1 = detail_soup.find('h1')
                    title = h1.get_text(strip=True) if h1 else preview_title
                    
                    # æ—¥æœŸ
                    date_text = None
                    for pattern in date_patterns:
                        match = re.search(pattern, detail_resp.text)
                        if match:
                            date_text = match.group(0)
                            break
                    
                    if not date_text or not is_within_30_days(date_text):
                        continue
                    
                    title = re.sub(r'20\d{2}[å¹´/.\-]\d{1,2}[æœˆ/.\-]\d{1,2}[æ—¥]?', '', title)
                    title = re.sub(r'\s+', ' ', title).strip()
                    
                    reports.append({
                        "Source": "CTBC",
                        "Date": date_text,
                        "Name": title,
                        "Link": pdf_url
                    })
                    
                    time.sleep(0.3)
                    
                except Exception:
                    continue
        
    except Exception as e:
        print(f"  âŒ CTBC çˆ¬å–å¤±æ•—: {e}")
    
    print(f"  âœ… CTBC æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
