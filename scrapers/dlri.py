import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½) - ğŸ•µï¸â€â™‚ï¸ å°åŒ…æ””æˆªèˆ‡å…§é ç›´æ“Šæ¨¡å¼...")
    reports = []
    
    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        print("  âŒ å°šæœªå®‰è£ Playwrightï¼Œè«‹ç¢ºèª requirements.txt")
        return reports

    report_urls = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        # å½è£æˆä¸€èˆ¬çš„ Windows Chrome ç€è¦½å™¨
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            viewport={'width': 1920, 'height': 1080}
        )
        page = context.new_page()
        
        # ğŸŒŸ é­”æ³•æ””æˆªå™¨ï¼šç›£è½ç€è¦½å™¨åœ¨èƒŒæ™¯æ”¶ç™¼çš„æ‰€æœ‰ç¶²è·¯å°åŒ…
        def handle_response(response):
            # å¦‚æœç™¼ç¾é€™æ˜¯ MarsFlag API å‚³å›ä¾†çš„è³‡æ–™å¤¾
            if "finder.api.mf.marsflag.com" in response.url:
                try:
                    text = response.text()
                    text = text.replace('\\/', '/') # æŠŠ JSON è£¡çš„è·³è„«æ–œç·šé‚„åŸ
                    
                    # æš´åŠ›è§£æï¼šç›´æ¥å¾å°åŒ…å­—ä¸²è£¡ï¼ŒæŒ–å‡ºæ‰€æœ‰ DLRI å ±å‘Šçš„ç¶²å€
                    urls = re.findall(r'https?://www\.dlri\.co\.jp/report/[a-zA-Z0-9_/-]+\.html', text)
                    for u in urls:
                        report_urls.add(u)
                except:
                    pass
                    
        # æ›ä¸Šæ””æˆªå™¨
        page.on("response", handle_response)
        
        try:
            # è®“ç€è¦½å™¨çœŸæ­£å»é€ è¨ªç¶²é ï¼Œè§¸ç™¼ CloudFront æ”¾è¡Œèˆ‡ API è«‹æ±‚
            page.goto("https://www.dlri.co.jp/report_index.html", wait_until="networkidle", timeout=20000)
            page.wait_for_timeout(4000) # çµ¦ç¶²é  4 ç§’é˜çš„æ™‚é–“ä¸‹è¼‰å°åŒ…
            
            # å‚™ç”¨æ–¹æ¡ˆï¼šä¹Ÿé †ä¾¿æŠ“å–ç•«é¢ä¸Šèƒ½çœ‹åˆ°çš„å‚³çµ±é€£çµ
            hrefs = page.evaluate("""() => Array.from(document.querySelectorAll('a')).map(a => a.href)""")
            for href in hrefs:
                if "/report/" in href and href.endswith('.html'):
                    report_urls.add(href)
        except Exception as e:
            print(f"  âŒ Playwright åŸ·è¡Œéç¨‹ç™¼ç”Ÿè¶…æ™‚: {e}")
        finally:
            browser.close()

    # æ¸…ç†ä¸è¦çš„é›œè¨Šç¶²å€
    clean_urls = set()
    for u in report_urls:
        if not any(kw in u for kw in ["report_index", "category", "type", "tag"]):
            clean_urls.add(u)

    print(f"  [åµæ¢å›å ±] æˆåŠŸæ””æˆªåˆ° {len(clean_urls)} å€‹çœŸå¯¦å ±å‘Šç¶²å€ï¼Œæº–å‚™é€²å…¥å…§é æå–...")

    # ==========================================
    # é€ä¸€é€²å…¥å ±å‘Šå…§é ï¼Œç„¡è¦–é¦–é æ’ç‰ˆï¼Œç›´æ¥æŠ“å–ç²¾ç¢ºè³‡æ–™
    # ==========================================
    for url in clean_urls:
        try:
            # DLRI çš„å…§é æ²’æœ‰æ“‹ä¸€èˆ¬çˆ¬èŸ²ï¼Œæˆ‘å€‘ç”¨è¼•é‡çš„ requests å¿«é€ŸæŠ“å–
            resp = requests.get(url, headers=HEADERS, timeout=5)
            resp.encoding = 'utf-8'
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # 1. æŠ“æ¨™é¡Œ (é€šå¸¸ <title> æ¨™ç±¤æ˜¯æœ€ä¹¾æ·¨çš„)
            title_tag = soup.find('title')
            if not title_tag: continue
            title = title_tag.get_text(strip=True).split('|')[0].strip()
            
            # æ’é™¤éå ±å‘Šçš„ç¶²é 
            if len(title) < 5 or any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…"]): continue
            
            # 2. æŠ“æ—¥æœŸ (æš´åŠ›åœ¨åŸå§‹ç¢¼å…§å°‹æ‰¾æ—¥æœŸæ ¼å¼ï¼Œå®¹è¨±ç©ºç™½)
            date_text = None
            date_match = re.search(r'20\d{2}\s*[./å¹´]\s*\d{1,2}\s*[./æœˆ]\s*\d{1,2}', resp.text)
            if date_match:
                date_text = date_match.group(0)
                
            # åš´æ ¼å®ˆé–€å“¡ï¼šæ²’æ—¥æœŸæˆ–è¶…é30å¤©å°±è¸¢æ‰
            if not date_text or not is_within_30_days(date_text):
                continue
                
            # 3. æ‰¾ PDF ä¸‹è¼‰é€£çµ
            pdf_tag = soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
            if not pdf_tag:
                pdf_tag = soup.find('a', string=re.compile(r'PDF', re.IGNORECASE))
                
            final_pdf = urljoin(url, pdf_tag['href']) if (pdf_tag and pdf_tag.get('href')) else url
            
            reports.append({
                "Source": "DLRI", 
                "Date": date_text, 
                "Name": title, 
                "Link": final_pdf
            })
            time.sleep(0.3) # ç¦®è²Œæ€§å»¶é²
        except Exception as e:
            pass

    print(f"  âœ… DLRI æœ€çµ‚æˆåŠŸæ”¶éŒ„ {len(reports)} ç­†å ±å‘Š")
    return reports
