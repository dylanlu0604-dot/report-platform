import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days, fetch_real_pdf_link

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€)...")
    base_url = "https://www.dlri.co.jp"
    
    # ğŸŒŸ ä¿®æ­£ï¼šé€™æ˜¯ DLRI çœŸæ­£çš„å ±å‘Šæ¸…å–®æ­£ç¢ºç¶²å€
    target_url = "https://www.dlri.co.jp/report_index.html"
    reports = []
    
    try:
        resp = requests.get(target_url, headers=HEADERS, timeout=15)
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # æŠ“å–æ‰€æœ‰é€£çµ
        links = soup.find_all('a', href=True)
        seen_hrefs = set()
        
        for tag in links:
            href = tag['href']
            title = tag.get_text(strip=True)
            
            # 1. æ’é™¤å¤ªçŸ­æˆ–é‡è¤‡çš„é€£çµ
            if len(title) < 5 or href in seen_hrefs: continue
            
            # 2. ç¢ºä¿ç¶²å€ç‰¹å¾µæ˜¯å ±å‘Š (åŒ…å« /report/ ä¸”ä»¥ .html çµå°¾)
            if "/report/" not in href or not href.endswith('.html'): continue
            if any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›", "æ¤œç´¢"]): continue
            
            # 3. æ‰¾æ—¥æœŸ (DLRI çš„æ—¥æœŸæ ¼å¼é€šå¸¸ç‚º 2026.02.16)
            date_text = None
            parent = tag.find_parent()
            if parent:
                # æŠŠæ¨™ç±¤å’Œå®ƒå‰å¾Œçš„æ–‡å­—åˆåœ¨ä¸€èµ·æ‰¾
                txt = parent.get_text()
                prev = parent.find_previous_sibling()
                if prev: txt += " " + prev.get_text()
                
                match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', txt)
                if match:
                    date_text = match.group(0)
            
            # 4. æ‰¾ä¸åˆ°æ—¥æœŸæˆ–è¶…é30å¤©å°±è¸¢é™¤
            if not date_text: continue
            if not is_within_30_days(date_text): continue
            
            # 5. åŠ å…¥æ¸…å–®
            seen_hrefs.add(href)
            link = urljoin(base_url, href)
            final_pdf = fetch_real_pdf_link(link)
            
            reports.append({
                "Source": "DLRI", 
                "Date": date_text, 
                "Name": title, 
                "Link": final_pdf
            })
            
    except Exception as e:
        print(f"  âŒ DLRI å¤±æ•—: {e}")
    
    print(f"  âœ… DLRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
