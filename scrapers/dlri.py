import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days, fetch_real_pdf_link

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€)...")
    base_url = "https://www.dlri.co.jp"
    
    # ğŸŒŸ ç ´è§£æ³•ï¼šé¿é–‹ JavaScript å‹•æ…‹è¼‰å…¥çš„é¦–é ï¼Œç›´æ¥å»æŠ“é€™ 7 å€‹éœæ…‹åˆ†é¡æ¸…å–®ï¼
    target_urls = [
        "https://www.dlri.co.jp/summary/type/trends.html",
        "https://www.dlri.co.jp/summary/type/indicators.html",
        "https://www.dlri.co.jp/summary/type/forecast.html",
        "https://www.dlri.co.jp/summary/type/market.html",
        "https://www.dlri.co.jp/summary/type/life_design.html",
        "https://www.dlri.co.jp/summary/type/dlri_report.html",
        "https://www.dlri.co.jp/summary/type/businessenvironment.html"
    ]
    
    reports = []
    seen_hrefs = set()
    
    for target_url in target_urls:
        try:
            resp = requests.get(target_url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(resp.content, 'html.parser')
            links = soup.find_all('a', href=True)
            
            for tag in links:
                href = tag['href']
                title = tag.get_text(strip=True)
                
                # 1. æ’é™¤å¤ªçŸ­æˆ–å·²ç¶“æŠ“éçš„é€£çµ
                if len(title) < 5 or href in seen_hrefs: continue
                
                # 2. ç¢ºèªæ˜¯å ±å‘Š (å¿…é ˆåŒ…å« /report/ )
                if "/report/" not in href: continue
                if any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›", "æ¤œç´¢"]): continue
                if href.endswith('/') or href.endswith('index.html'): continue
                
                # 3. æ‰¾æ—¥æœŸ
                date_text = None
                parent = tag.find_parent()
                if parent:
                    txt = parent.get_text()
                    prev = parent.find_previous_sibling()
                    if prev: txt += " " + prev.get_text()
                    
                    match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', txt)
                    if match:
                        date_text = match.group(0)
                
                # 4. åš´æ ¼éæ¿¾ï¼šæ²’æ—¥æœŸã€æˆ–è¶…é 30 å¤©ï¼Œç«‹åˆ»è¸¢é™¤
                if not date_text: continue
                if not is_within_30_days(date_text): continue
                
                # 5. åŠ å…¥æ¸…å–®
                seen_hrefs.add(href)
                link = urljoin(base_url, href)
                final_pdf = fetch_real_pdf_link(link)
                
                reports.append({
                    "Source": "DLRI", 
                    "Date": date_text, 
                    "Name": title, 
                    "Link": final_pdf
                })
                
        except Exception as e:
            print(f"  âŒ DLRI ({target_url}) å¤±æ•—: {e}")
            
    print(f"  âœ… DLRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
