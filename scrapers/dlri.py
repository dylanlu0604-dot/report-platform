import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    DLRI çˆ¬èŸ² - çµ‚æ¥µéˆæ´»ç‰ˆ
    
    æ ¸å¿ƒç­–ç•¥: ä¸å‡è¨­ä»»ä½•ç‰¹å®šçš„ URL æ ¼å¼,è€Œæ˜¯:
    1. æƒæé¦–é æ‰¾å‡ºæ‰€æœ‰å¯èƒ½æ˜¯"å ±å‘Šåˆ—è¡¨"çš„é€£çµ
    2. å¾é€™äº›åˆ—è¡¨é æ‰¾å‡ºæ‰€æœ‰å ±å‘Š
    3. è¨ªå•æ¯å€‹å ±å‘Šé é¢æå–è³‡è¨Š
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€) - ğŸ¯ çµ‚æ¥µéˆæ´»æ¨¡å¼...")
    reports = []
    seen_urls = set()
    
    base_url = "https://www.dlri.co.jp"
    
    # çµ±è¨ˆ
    stats = {
        'list_pages_found': 0,
        'report_candidates': 0,
        'valid_reports': 0,
        'no_date': 0,
        'old_date': 0
    }
    
    # ==========================================
    # éšæ®µ 1: å¾é¦–é æ‰¾å‡º"å ±å‘Šåˆ—è¡¨é "çš„é€£çµ
    # ==========================================
    print("  [éšæ®µ 1] å¾é¦–é å°‹æ‰¾å ±å‘Šåˆ—è¡¨é ...")
    
    list_page_urls = []
    
    try:
        resp = requests.get(base_url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        # ç­–ç•¥: æ‰¾æ‰€æœ‰å…§éƒ¨é€£çµ,éæ¿¾å‡ºå¯èƒ½æ˜¯å ±å‘Šåˆ—è¡¨çš„
        for link in soup.find_all('a', href=True):
            href = link['href']
            text = link.get_text(strip=True)
            
            # è½‰æ›ç‚ºçµ•å° URL
            if href.startswith('/'):
                full_url = urljoin(base_url, href)
            elif href.startswith('http'):
                full_url = href
            else:
                continue
            
            # åªè™•ç† DLRI ç¶²ç«™çš„é€£çµ
            if 'dlri.co.jp' not in full_url:
                continue
            
            # é—œéµ: æ‰¾å¯èƒ½åŒ…å«å ±å‘Šåˆ—è¡¨çš„é é¢
            # æ—¥æ–‡é—œéµå­—: ãƒ¬ãƒãƒ¼ãƒˆ(report)ã€èª¿æŸ»ç ”ç©¶(research)ã€åˆŠè¡Œç‰©(publications)
            keywords = [
                'report', 'research', 'publication', 'column', 'article',
                'ãƒ¬ãƒãƒ¼ãƒˆ', 'å ±å‘Š', 'èª¿æŸ»', 'ç ”ç©¶', 'åˆŠè¡Œ', 'ã‚³ãƒ©ãƒ ',
                'macro', 'market', 'economy', 'finance', 'outlook',
                'ãƒã‚¯ãƒ­', 'å¸‚å ´', 'çµŒæ¸ˆ', 'é‡‘è', 'å±•æœ›'
            ]
            
            # æª¢æŸ¥ URL æˆ–é€£çµæ–‡å­—æ˜¯å¦åŒ…å«é€™äº›é—œéµå­—
            url_lower = full_url.lower()
            text_lower = text.lower()
            
            if any(kw in url_lower or kw in text_lower for kw in keywords):
                # æ’é™¤æ˜é¡¯ä¸æ˜¯åˆ—è¡¨çš„é é¢
                if not any(exclude in text for exclude in ['English', 'ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼', 'ã‚µã‚¤ãƒˆãƒãƒƒãƒ—']):
                    list_page_urls.append(full_url)
        
        # å»é‡
        list_page_urls = list(set(list_page_urls))
        stats['list_pages_found'] = len(list_page_urls)
        
        print(f"    âœ“ æ‰¾åˆ° {len(list_page_urls)} å€‹å¯èƒ½çš„å ±å‘Šåˆ—è¡¨é ")
        
        # é¡¯ç¤ºå‰å¹¾å€‹ä¾›é™¤éŒ¯
        if list_page_urls:
            print(f"    ğŸ“‹ æ¨£æœ¬åˆ—è¡¨é :")
            for url in list_page_urls[:5]:
                print(f"       â€¢ {url}")
                
    except Exception as e:
        print(f"    âœ— é¦–é æƒæå¤±æ•—: {e}")
        # å³ä½¿é¦–é å¤±æ•—,ä¹Ÿå˜—è©¦ä¸€äº›å·²çŸ¥çš„å¸¸è¦‹è·¯å¾‘
        list_page_urls = [
            f"{base_url}/report",
            f"{base_url}/research", 
            f"{base_url}/column",
            f"{base_url}/macro",
        ]
    
    # ==========================================
    # éšæ®µ 2: å¾åˆ—è¡¨é æ‰¾å‡ºæ‰€æœ‰å ±å‘Šé€£çµ
    # ==========================================
    print(f"\n  [éšæ®µ 2] å¾åˆ—è¡¨é æå–å ±å‘Šé€£çµ...")
    
    report_urls = []
    
    for list_url in list_page_urls[:20]:  # é™åˆ¶æœ€å¤šæª¢æŸ¥ 20 å€‹åˆ—è¡¨é 
        try:
            resp = requests.get(list_url, headers=HEADERS, timeout=8)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # ç­–ç•¥: æ‰¾æ‰€æœ‰çœ‹èµ·ä¾†åƒ"å ±å‘Šè©³æƒ…é "çš„é€£çµ
            for link in soup.find_all('a', href=True):
                href = link['href']
                text = link.get_text(strip=True)
                
                # è½‰æ›ç‚ºçµ•å° URL
                full_url = urljoin(list_url, href)
                
                # åªè¦ DLRI çš„é€£çµ
                if 'dlri.co.jp' not in full_url:
                    continue
                
                # é—œéµ: éæ¿¾æ¢ä»¶è¦éå¸¸å¯¬é¬†
                # åªæ’é™¤æ˜é¡¯ä¸æ˜¯å ±å‘Šçš„é€£çµ
                exclude_patterns = [
                    r'mailto:', r'javascript:', r'#',
                    r'\.pdf$', r'\.zip$', r'\.xlsx?$',  # é€™äº›æ˜¯ä¸‹è¼‰æª”æ¡ˆ,ä¸æ˜¯å ±å‘Šé 
                    r'/(english|privacy|sitemap|contact|about)/',
                ]
                
                if any(re.search(pattern, full_url, re.I) for pattern in exclude_patterns):
                    continue
                
                # é€£çµæ–‡å­—å¤ªçŸ­æˆ–æ˜¯å°èˆªæ–‡å­—å°±è·³é
                if len(text) < 5:
                    continue
                    
                nav_keywords = [
                    'ä¸€è¦§', 'List', 'ãƒˆãƒƒãƒ—', 'ãƒ›ãƒ¼ãƒ ', 'Home', 'TOP',
                    'æ¬¡ã¸', 'å‰ã¸', 'Next', 'Previous', 'ã‚‚ã£ã¨è¦‹ã‚‹',
                    'æˆ»ã‚‹', 'Back', 'æ¤œç´¢', 'Search', 'ãŠçŸ¥ã‚‰ã›', 'News'
                ]
                
                if any(kw in text for kw in nav_keywords):
                    continue
                
                # çœ‹èµ·ä¾†æ˜¯æœ‰æ•ˆçš„å ±å‘Šé€£çµ
                report_urls.append((full_url, text))
                
            time.sleep(0.2)  # ç¦®è²Œæ€§å»¶é²
            
        except Exception as e:
            continue
    
    # å»é‡
    report_urls = list(set(report_urls))
    stats['report_candidates'] = len(report_urls)
    
    print(f"    âœ“ æ‰¾åˆ° {len(report_urls)} å€‹å ±å‘Šå€™é¸é€£çµ")
    
    if len(report_urls) == 0:
        print(f"\n  âš ï¸  æ²’æœ‰æ‰¾åˆ°ä»»ä½•å ±å‘Šé€£çµ!")
        print(f"  ğŸ” å¯èƒ½çš„åŸå› :")
        print(f"     1. DLRI ç¶²ç«™å®Œå…¨æ”¹ç‰ˆäº†")
        print(f"     2. å ±å‘Šå…§å®¹éœ€è¦ç™»å…¥æ‰èƒ½çœ‹")
        print(f"     3. ä½¿ç”¨ JavaScript å‹•æ…‹è¼‰å…¥ (éœ€è¦ Selenium)")
        print(f"\n  ğŸ’¡ å»ºè­°:")
        print(f"     1. æ‰‹å‹•è¨ªå• https://www.dlri.co.jp/ ç¢ºèªå ±å‘Šä½ç½®")
        print(f"     2. æª¢æŸ¥æ˜¯å¦éœ€è¦ç™»å…¥")
        print(f"     3. æŸ¥çœ‹ç¶²é åŸå§‹ç¢¼,æœå°‹ 'report' æˆ– 'ãƒ¬ãƒãƒ¼ãƒˆ'")
        print(f"\n  âœ… DLRI æœ€çµ‚æˆåŠŸæ”¶éŒ„ {len(reports)} ç­†å ±å‘Š")
        return reports
    
    # é¡¯ç¤ºæ¨£æœ¬
    print(f"    ğŸ“‹ å ±å‘Šæ¨£æœ¬:")
    for url, title in report_urls[:5]:
        print(f"       â€¢ {title[:50]}")
        print(f"         {url}")
    
    # ==========================================
    # éšæ®µ 3: è¨ªå•æ¯å€‹å ±å‘Šé é¢æå–è©³ç´°è³‡è¨Š
    # ==========================================
    print(f"\n  [éšæ®µ 3] æå–å ±å‘Šè©³æƒ… (0/{len(report_urls)})...", end='', flush=True)
    
    for idx, (url, title_preview) in enumerate(report_urls, 1):
        # é¿å…é‡è¤‡
        if url in seen_urls:
            continue
        seen_urls.add(url)
        
        # é€²åº¦é¡¯ç¤º
        if idx % 5 == 0:
            print(f"\r  [éšæ®µ 3] æå–å ±å‘Šè©³æƒ… ({idx}/{len(report_urls)})...", end='', flush=True)
        
        try:
            resp = requests.get(url, headers=HEADERS, timeout=8)
            resp.encoding = 'utf-8'
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æå–æ¨™é¡Œ (å„ªå…ˆé †åº: h1 > title > é è¦½æ¨™é¡Œ)
            title = None
            
            h1 = soup.find('h1')
            if h1:
                title = h1.get_text(strip=True)
            
            if not title or len(title) < 5:
                title_tag = soup.find('title')
                if title_tag:
                    title = title_tag.get_text(strip=True).split('|')[0].split('ï½œ')[0].strip()
            
            if not title or len(title) < 5:
                title = title_preview
            
            # æœ€çµ‚æª¢æŸ¥
            if not title or len(title) < 5:
                continue
            
            # æå–æ—¥æœŸ (åœ¨æ•´å€‹é é¢ä¸­æœå°‹)
            date_text = None
            
            # æ—¥æœŸæ­£å‰‡æ¨¡å¼ (æ”¯æ´å¤šç¨®æ ¼å¼)
            date_patterns = [
                r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',      # 2026å¹´2æœˆ16æ—¥
                r'20\d{2}\.\d{1,2}\.\d{1,2}',        # 2026.2.16
                r'20\d{2}/\d{1,2}/\d{1,2}',          # 2026/2/16
                r'20\d{2}-\d{1,2}-\d{1,2}',          # 2026-02-16
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, resp.text)
                if match:
                    date_text = match.group(0)
                    break
            
            # æ²’æ—¥æœŸå°±è·³é
            if not date_text:
                stats['no_date'] += 1
                continue
            
            # æª¢æŸ¥æ˜¯å¦åœ¨ 30 å¤©å…§
            if not is_within_30_days(date_text):
                stats['old_date'] += 1
                continue
            
            # å°‹æ‰¾ PDF é€£çµ
            pdf_link = url  # é è¨­
            
            # å¤šç¨®ç­–ç•¥å°‹æ‰¾ PDF
            pdf_candidates = []
            
            # ç­–ç•¥ 1: href åŒ…å« .pdf
            for a in soup.find_all('a', href=re.compile(r'\.pdf$', re.I)):
                pdf_candidates.append(urljoin(url, a['href']))
            
            # ç­–ç•¥ 2: æ–‡å­—åŒ…å« PDF æˆ–ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            for a in soup.find_all('a', string=re.compile(r'PDF|ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰|download', re.I)):
                if a.get('href'):
                    pdf_candidates.append(urljoin(url, a['href']))
            
            # ç­–ç•¥ 3: class/id åŒ…å« pdf
            for a in soup.find_all('a', attrs={'class': re.compile(r'pdf', re.I)}):
                if a.get('href'):
                    pdf_candidates.append(urljoin(url, a['href']))
            
            # ä½¿ç”¨ç¬¬ä¸€å€‹æ‰¾åˆ°çš„ PDF
            if pdf_candidates:
                pdf_link = pdf_candidates[0]
            
            # åŠ å…¥çµæœ
            reports.append({
                "Source": "DLRI",
                "Date": date_text,
                "Name": title,
                "Link": pdf_link
            })
            
            stats['valid_reports'] += 1
            
            time.sleep(0.15)  # ç¦®è²Œæ€§å»¶é²
            
        except Exception as e:
            continue
    
    print(f"\r  [éšæ®µ 3] æå–å ±å‘Šè©³æƒ… ({len(report_urls)}/{len(report_urls)}) âœ“")
    
    # ==========================================
    # çµ±è¨ˆå ±å‘Š
    # ==========================================
    print(f"\n  ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"     åˆ—è¡¨é æ•¸: {stats['list_pages_found']}")
    print(f"     å ±å‘Šå€™é¸: {stats['report_candidates']}")
    print(f"     æœ‰æ•ˆå ±å‘Š: {stats['valid_reports']}")
    print(f"     ç„¡æ—¥æœŸ: {stats['no_date']}")
    print(f"     èˆŠå ±å‘Š: {stats['old_date']}")
    print(f"  âœ… DLRI æœ€çµ‚æˆåŠŸæ”¶éŒ„ {len(reports)} ç­†å ±å‘Š")
    
    return reports


if __name__ == "__main__":
    results = scrape()
    
    if results:
        print(f"\nğŸ“„ æ‰¾åˆ°çš„å ±å‘Š:")
        for i, r in enumerate(results, 1):
            print(f"\n{i}. [{r['Date']}] {r['Name'][:70]}")
            print(f"   ğŸ”— {r['Link']}")
