import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days, fetch_real_pdf_link

def scrape():
    print("\nğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€) - ğŸ•µï¸ åµæ¢æ¨¡å¼å•Ÿå‹•...")
    base_url = "https://www.dlri.co.jp"
    target_url = "https://www.dlri.co.jp/report/"
    reports = []
    
    try:
        resp = requests.get(target_url, headers=HEADERS, timeout=15)
        print(f"  [åµæ¢å›å ±] ğŸŒ HTTP ç‹€æ…‹ç¢¼: {resp.status_code} (å¦‚æœæ˜¯ 200 ä»£è¡¨æˆåŠŸé€²å…¥ç¶²ç«™)")
        
        soup = BeautifulSoup(resp.content, 'html.parser')
        links = soup.find_all('a', href=True)
        print(f"  [åµæ¢å›å ±] ğŸ“„ ç¶²é ä¸­ç¸½å…±æ‰¾åˆ°äº† {len(links)} å€‹ <a> é€£çµæ¨™ç±¤")
        
        seen_hrefs = set()
        
        for tag in links:
            href = tag['href']
            title = tag.get_text(strip=True)
            
            # å°‹æ‰¾ç¶²å€è£¡æœ‰ /report/ çš„é€£çµ
            if "/report/" not in href:
                continue
                
            # æ’é™¤æ˜é¡¯æ˜¯å°è¦½åˆ—çš„ç„¡ç”¨é€£çµ
            if any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›"]): 
                continue
                
            if href in seen_hrefs: continue
            seen_hrefs.add(href)
            
            print(f"  ----------------------------------------")
            print(f"  [åµæ¢å›å ±] ğŸ¯ ç™¼ç¾å€™é¸å ±å‘Š: {title[:30]}... ({href})")
            
            # é–‹å§‹æ‰¾æ—¥æœŸ
            date_text = "æœªçŸ¥æ—¥æœŸ"
            parent = tag.find_parent()
            if parent:
                parent_text = parent.get_text()
                prev = parent.find_previous_sibling()
                if prev:
                    parent_text += " " + prev.get_text()
                
                match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}æ—¥?', parent_text)
                if match: 
                    date_text = match.group(0)
            
            print(f"  [åµæ¢å›å ±] ğŸ“… è§£æå‡ºçš„æ—¥æœŸ: {date_text}")
            
            if date_text != "æœªçŸ¥æ—¥æœŸ":
                if is_within_30_days(date_text):
                    link = urljoin(base_url, href)
                    final_pdf = fetch_real_pdf_link(link)
                    reports.append({
                        "Source": "DLRI", 
                        "Date": date_text, 
                        "Name": title, 
                        "Link": final_pdf
                    })
                    print(f"    âœ”ï¸ æˆåŠŸåŠ å…¥æ¸…å–®ï¼")
                else:
                    print(f"    âŒ è¢«è¸¢é™¤ (åŸå› : æ—¥æœŸè¶…é 30 å¤©)")
            else:
                print(f"    âŒ è¢«è¸¢é™¤ (åŸå› : æ‰¾ä¸åˆ°æœ‰æ•ˆæ—¥æœŸæ ¼å¼)")
                
    except Exception as e:
        print(f"  âŒ DLRI å¤±æ•—: {e}")
    
    print(f"  âœ… DLRI æœ€çµ‚æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š\n")
    return reports
