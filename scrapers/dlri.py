import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days, fetch_real_pdf_link
import time

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€)...")
    base_url = "https://www.dlri.co.jp"
    # æ”¹ç‚ºæŠ“å–é¦–é èˆ‡å ±å‘Šç¸½è¦½é ï¼Œé€™å…©å€‹åœ°æ–¹æœ€æ–°å ±å‘Šæœ€é½Šå…¨
    target_urls = [
        "https://www.dlri.co.jp/",
        "https://www.dlri.co.jp/report_index.html"
    ]
    reports = []
    seen_hrefs = set()
    
    for target_url in target_urls:
        try:
            resp = requests.get(target_url, headers=HEADERS, timeout=15)
            soup = BeautifulSoup(resp.content, 'html.parser')
            links = soup.find_all('a', href=True)
            
            for tag in links:
                href = tag['href']
                title = tag.get_text(strip=True)
                
                # 1. åŸºæœ¬æ’é™¤
                if len(title) < 5 or href in seen_hrefs: continue
                
                # 2. é™å®šå¿…é ˆæ˜¯å ±å‘Šçš„ç¶²å€ (åŒ…å« /report/ ä¸”é€šå¸¸ä»¥ .html çµå°¾)
                if "/report/" not in href or not href.endswith('.html'): continue
                if any(kw in href for kw in ["report_index", "category", "type"]): continue
                if any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›"]): continue
                
                # 3. æ‰¾æ—¥æœŸï¼šå…ˆå¾å¤–å±¤å®¹å™¨ (li, dl, tr, div) æ‰¾æ‰¾çœ‹
                date_text = None
                parent = tag.find_parent(['li', 'dl', 'tr', 'div'])
                if parent:
                    match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', parent.get_text())
                    if match:
                        date_text = match.group(0)
                
                link = urljoin(base_url, href)
                
                # 4. çµ‚æ¥µå¿…æ®ºæŠ€ï¼šå¦‚æœåœ¨å¤–é¢æ‰¾ä¸åˆ°æ—¥æœŸï¼Œç›´æ¥é»é€²å»å…§é æ‰¾ï¼
                if not date_text:
                    try:
                        detail_resp = requests.get(link, headers=HEADERS, timeout=5)
                        detail_match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', detail_resp.text)
                        if detail_match:
                            date_text = detail_match.group(0)
                        time.sleep(0.3) # ç¦®è²Œæ€§æš«åœï¼Œä¸è¦æŠŠäººå®¶ç¶²ç«™æ‰“æ›
                    except:
                        pass
                
                # 5. å¦‚æœé€£å…§é éƒ½æ‰¾ä¸åˆ°æ—¥æœŸï¼Œé‚£å°±çœŸçš„ä¸æ˜¯å ±å‘Š
                if not date_text: continue
                
                # 6. è¶…é 30 å¤©å°±è·³é
                if not is_within_30_days(date_text): continue

                seen_hrefs.add(href)
                final_pdf = fetch_real_pdf_link(link)
                
                reports.append({
                    "Source": "DLRI", 
                    "Date": date_text, 
                    "Name": title, 
                    "Link": final_pdf
                })
                    
        except Exception as e:
            print(f"  âŒ DLRI ({target_url}) å¤±æ•—: {e}")
    
    print(f"  âœ… DLRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
