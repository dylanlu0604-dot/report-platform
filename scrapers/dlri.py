import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    DLRI çˆ¬èŸ² - ä¸‰å¼•æ“å¢å¼·ç‰ˆ
    
    ç­–ç•¥èªªæ˜:
    1. MarsFlag API (å¦‚æœå¯ç”¨)
    2. é¦–é é€£çµæƒæ
    3. å·²çŸ¥å ±å‘Šåˆ—è¡¨é é¢ (å‚™æ´)
    4. RSS/Sitemap (å¦‚æœæœ‰)
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€) - ğŸš€ ä¸‰å¼•æ“å¢å¼·æ¨¡å¼...")
    reports = []
    seen_urls = set()
    found_paths = []
    
    # çµ±è¨ˆè³‡è¨Š
    stats = {
        'api_paths': 0,
        'homepage_paths': 0,
        'list_page_paths': 0,
        'valid_reports': 0,
        'no_date': 0,
        'old_date': 0,
        'errors': 0
    }

    # ==========================================
    # å¼•æ“ 1ï¼šMarsFlag API (å¯èƒ½éœ€è¦èªè­‰æˆ–æœ‰é€Ÿç‡é™åˆ¶)
    # ==========================================
    print("  [å¼•æ“ 1] å˜—è©¦ MarsFlag API...")
    api_url = "https://finder.api.mf.marsflag.com/api/v1/finder_service/documents/d3eff4d6/search"
    api_headers = HEADERS.copy()
    api_headers.update({
        'Accept': 'application/json, text/plain, */*',
        'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
        'Origin': 'https://www.dlri.co.jp',
        'Referer': 'https://www.dlri.co.jp/',
        'Sec-Fetch-Dest': 'empty',
        'Sec-Fetch-Mode': 'cors',
        'Sec-Fetch-Site': 'cross-site'
    })
    
    # å˜—è©¦å¤šç¨®æŸ¥è©¢ç­–ç•¥
    queries = [
        {"match": "çµŒæ¸ˆ", "q": "çµŒæ¸ˆ"},
        {"match": "å¸‚å ´", "q": "å¸‚å ´"},
        {"match": "é‡‘è", "q": "é‡‘è"},
        {"match": "ãƒ¬ãƒãƒ¼ãƒˆ", "q": "ãƒ¬ãƒãƒ¼ãƒˆ"},
        {"q": ""},  # ç©ºæŸ¥è©¢æœ‰æ™‚èƒ½è¿”å›æœ€æ–°é …ç›®
    ]
    
    for query_params in queries:
        try:
            resp = requests.get(api_url, params=query_params, headers=api_headers, timeout=8)
            if resp.status_code == 200:
                # æ¸…ç† JSON è·³è„«å­—å…ƒ
                text = resp.text.replace('\\/', '/')
                
                # æ¨¡å¼ 1: /report/xxx.html
                paths = re.findall(r'/report/[a-zA-Z0-9_/-]+\.html', text)
                found_paths.extend(paths)
                stats['api_paths'] += len(paths)
                
                # å¦‚æœæ‰¾åˆ°çµæœå°±ä¸éœ€è¦ç¹¼çºŒæŸ¥è©¢
                if paths:
                    break
        except Exception as e:
            stats['errors'] += 1
            # API å¤±æ•—ä¸è¦ä¸­æ–·æ•´å€‹æµç¨‹
            pass
    
    if stats['api_paths'] > 0:
        print(f"    âœ“ API æ‰¾åˆ° {stats['api_paths']} å€‹è·¯å¾‘")
    else:
        print(f"    âœ— API ç„¡çµæœæˆ–ç„¡æ³•è¨ªå•")

    # ==========================================
    # å¼•æ“ 2ï¼šé¦–é é€£çµæƒæ
    # ==========================================
    print("  [å¼•æ“ 2] æƒæé¦–é é€£çµ...")
    try:
        top_resp = requests.get("https://www.dlri.co.jp/", headers=HEADERS, timeout=8)
        if top_resp.status_code == 200:
            # æ¨¡å¼ 1: æ¨™æº–å ±å‘Šé€£çµ
            paths = re.findall(r'/report/[a-zA-Z0-9_/-]+\.html', top_resp.text)
            found_paths.extend(paths)
            stats['homepage_paths'] = len(paths)
            
            # æ¨¡å¼ 2: æœ‰æ™‚é€£çµå¯èƒ½åŒ…å«åœ¨ JavaScript è®Šæ•¸ä¸­
            js_paths = re.findall(r'["\'](/report/[^"\']+\.html)["\']', top_resp.text)
            found_paths.extend(js_paths)
            stats['homepage_paths'] += len(js_paths)
            
            print(f"    âœ“ é¦–é æ‰¾åˆ° {stats['homepage_paths']} å€‹è·¯å¾‘")
        else:
            print(f"    âœ— é¦–é è¨ªå•å¤±æ•— (HTTP {top_resp.status_code})")
    except Exception as e:
        print(f"    âœ— é¦–é è¨ªå•ç•°å¸¸")
        stats['errors'] += 1

    # ==========================================
    # å¼•æ“ 3ï¼šå·²çŸ¥å ±å‘Šåˆ—è¡¨é é¢ (å‚™æ´)
    # ==========================================
    print("  [å¼•æ“ 3] å˜—è©¦å ±å‘Šåˆ—è¡¨é ...")
    list_pages = [
        "https://www.dlri.co.jp/report_index.html",
        "https://www.dlri.co.jp/report.html",
        "https://www.dlri.co.jp/report/index.html",
    ]
    
    for list_url in list_pages:
        try:
            resp = requests.get(list_url, headers=HEADERS, timeout=8)
            if resp.status_code == 200:
                paths = re.findall(r'/report/[a-zA-Z0-9_/-]+\.html', resp.text)
                if paths:
                    found_paths.extend(paths)
                    stats['list_page_paths'] += len(paths)
                    print(f"    âœ“ {list_url} æ‰¾åˆ° {len(paths)} å€‹è·¯å¾‘")
                    break  # æ‰¾åˆ°ä¸€å€‹æœ‰æ•ˆé é¢å°±å¤ äº†
        except:
            continue
    
    if stats['list_page_paths'] == 0:
        print(f"    âœ— ç„¡æ³•è¨ªå•ä»»ä½•åˆ—è¡¨é ")

    # ==========================================
    # è³‡æ–™è™•ç†ï¼šè¨ªå•æ¯å€‹å ±å‘Šé é¢ä¸¦æå–è³‡è¨Š
    # ==========================================
    unique_paths = list(set(found_paths))
    print(f"\n  ğŸ“Š ç¸½å…±æ”¶é›†åˆ° {len(unique_paths)} å€‹ä¸é‡è¤‡çš„å ±å‘Šé€£çµ")
    
    if len(unique_paths) == 0:
        print(f"  âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å ±å‘Šé€£çµ,å¯èƒ½åŸå› :")
        print(f"      1. ç¶²ç«™çµæ§‹å·²æ”¹è®Š")
        print(f"      2. API éœ€è¦èªè­‰æˆ–æœ‰ CORS é™åˆ¶")
        print(f"      3. æœ€è¿‘çœŸçš„æ²’æœ‰æ–°å ±å‘Š")
        print(f"  âœ… DLRI æœ€çµ‚æˆåŠŸæ”¶éŒ„ {len(reports)} ç­†å ±å‘Š")
        return reports
    
    print(f"  ğŸ” é–‹å§‹é€ä¸€æª¢é©—å ±å‘Šå…§é ...")
    
    for idx, path in enumerate(unique_paths, 1):
        url = urljoin("https://www.dlri.co.jp", path)
        
        # é¿å…é‡è¤‡è™•ç†
        if url in seen_urls:
            continue
        seen_urls.add(url)
        
        # é¡¯ç¤ºé€²åº¦ (æ¯ 10 å€‹é¡¯ç¤ºä¸€æ¬¡)
        if idx % 10 == 0:
            print(f"    è™•ç†ä¸­... {idx}/{len(unique_paths)}")
        
        try:
            detail_resp = requests.get(url, headers=HEADERS, timeout=8)
            detail_resp.encoding = 'utf-8'
            soup = BeautifulSoup(detail_resp.text, 'html.parser')
            
            # 1. æå–æ¨™é¡Œ
            title = None
            
            # ç­–ç•¥ A: <title> æ¨™ç±¤
            title_tag = soup.find('title')
            if title_tag:
                title = title_tag.get_text(strip=True)
                # æ¸…ç†æ¨™é¡Œ (ç§»é™¤ç¶²ç«™åç¨±)
                title = title.split('|')[0].split('ï½œ')[0].strip()
            
            # ç­–ç•¥ B: <h1> æ¨™ç±¤ (é€šå¸¸æ˜¯æ–‡ç« æ¨™é¡Œ)
            if not title or len(title) < 5:
                h1_tag = soup.find('h1')
                if h1_tag:
                    title = h1_tag.get_text(strip=True)
            
            # ç­–ç•¥ C: meta title
            if not title or len(title) < 5:
                meta_title = soup.find('meta', property='og:title')
                if meta_title and meta_title.get('content'):
                    title = meta_title['content'].strip()
            
            # ç„¡æ•ˆæ¨™é¡Œå°±è·³é
            if not title or len(title) < 5:
                continue
            
            # æ’é™¤æ˜é¡¯çš„å°èˆªé é¢
            if any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›", "æ¤œç´¢"]):
                continue
            
            # 2. æå–æ—¥æœŸ (å¤šç¨®ç­–ç•¥)
            date_text = None
            
            # ç­–ç•¥ A: åœ¨æ•´å€‹é é¢çš„åŸå§‹ç¢¼ä¸­æ‰¾ (æœ€ç²—æš´ä½†æœ‰æ•ˆ)
            date_patterns = [
                r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?',  # 2026.02.16, 2026å¹´2æœˆ16æ—¥
                r'20\d{2}[-]\d{1,2}[-]\d{1,2}',             # 2026-02-16
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, detail_resp.text)
                if match:
                    date_text = match.group(0)
                    break
            
            # ç­–ç•¥ B: æª¢æŸ¥ meta æ¨™ç±¤
            if not date_text:
                meta_date = soup.find('meta', property='article:published_time')
                if meta_date and meta_date.get('content'):
                    date_text = meta_date['content'][:10]  # é€šå¸¸æ˜¯ YYYY-MM-DD æ ¼å¼
            
            # æ²’æœ‰æ—¥æœŸå°±è·³é
            if not date_text:
                stats['no_date'] += 1
                continue
            
            # æª¢æŸ¥æ˜¯å¦åœ¨ 30 å¤©å…§
            if not is_within_30_days(date_text):
                stats['old_date'] += 1
                continue
            
            # 3. æ‰¾ PDF ä¸‹è¼‰é€£çµ
            pdf_link = url  # é è¨­å€¼
            
            # ç­–ç•¥ A: æ‰¾ <a> æ¨™ç±¤ä¸­åŒ…å« .pdf çš„é€£çµ
            pdf_tag = soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
            
            # ç­–ç•¥ B: æ‰¾æ–‡å­—åŒ…å« "PDF" çš„é€£çµ
            if not pdf_tag:
                pdf_tag = soup.find('a', string=re.compile(r'PDF|ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰|download', re.IGNORECASE))
            
            # ç­–ç•¥ C: æ‰¾ class æˆ– id åŒ…å« pdf çš„å…ƒç´ 
            if not pdf_tag:
                pdf_tag = soup.find('a', class_=re.compile(r'pdf', re.IGNORECASE))
            
            if pdf_tag and pdf_tag.get('href'):
                pdf_link = urljoin(url, pdf_tag['href'])
            
            # 4. åŠ å…¥çµæœ
            reports.append({
                "Source": "DLRI",
                "Date": date_text,
                "Name": title,
                "Link": pdf_link
            })
            stats['valid_reports'] += 1
            
            # ç¦®è²Œæ€§å»¶é²
            time.sleep(0.2)
            
        except Exception as e:
            stats['errors'] += 1
            # å–®å€‹é é¢å¤±æ•—ä¸å½±éŸ¿å…¶ä»–é é¢
            continue
    
    # è¼¸å‡ºçµ±è¨ˆè³‡è¨Š
    print(f"\n  ğŸ“Š çµ±è¨ˆè³‡è¨Š:")
    print(f"    - API è·¯å¾‘: {stats['api_paths']}")
    print(f"    - é¦–é è·¯å¾‘: {stats['homepage_paths']}")
    print(f"    - åˆ—è¡¨é è·¯å¾‘: {stats['list_page_paths']}")
    print(f"    - æœ‰æ•ˆå ±å‘Š: {stats['valid_reports']}")
    print(f"    - ç„¡æ—¥æœŸ: {stats['no_date']}")
    print(f"    - èˆŠæ—¥æœŸ: {stats['old_date']}")
    print(f"    - éŒ¯èª¤æ•¸: {stats['errors']}")
    print(f"  âœ… DLRI æœ€çµ‚æˆåŠŸæ”¶éŒ„ {len(reports)} ç­†å ±å‘Š")
    
    return reports


# ç¨ç«‹çš„æ¸¬è©¦å‡½æ•¸
def test_scraper():
    """æ¸¬è©¦å‡½æ•¸,ç”¨æ–¼è¨ºæ–·å•é¡Œ"""
    print("ğŸ§ª DLRI çˆ¬èŸ²æ¸¬è©¦æ¨¡å¼")
    print("=" * 60)
    
    # æ¸¬è©¦ 1: æª¢æŸ¥ç¶²ç«™æ˜¯å¦å¯è¨ªå•
    print("\n[æ¸¬è©¦ 1] ç¶²ç«™é€£é€šæ€§æ¸¬è©¦")
    try:
        resp = requests.get("https://www.dlri.co.jp/", headers=HEADERS, timeout=5)
        print(f"âœ“ é¦–é ç‹€æ…‹ç¢¼: {resp.status_code}")
        print(f"âœ“ å…§å®¹é•·åº¦: {len(resp.text)} å­—å…ƒ")
    except Exception as e:
        print(f"âœ— é¦–é è¨ªå•å¤±æ•—: {e}")
        return
    
    # æ¸¬è©¦ 2: æª¢æŸ¥æ˜¯å¦èƒ½æ‰¾åˆ°å ±å‘Šé€£çµ
    print("\n[æ¸¬è©¦ 2] é€£çµç™¼ç¾æ¸¬è©¦")
    paths = re.findall(r'/report/[a-zA-Z0-9_/-]+\.html', resp.text)
    print(f"é¦–é æ‰¾åˆ° {len(set(paths))} å€‹ä¸é‡è¤‡çš„å ±å‘Šé€£çµ")
    
    if paths:
        print("æ¨£æœ¬é€£çµ:")
        for p in list(set(paths))[:5]:
            print(f"  - https://www.dlri.co.jp{p}")
    
    # æ¸¬è©¦ 3: æ¸¬è©¦ä¸€å€‹å ±å‘Šé é¢
    if paths:
        print("\n[æ¸¬è©¦ 3] å–®ä¸€å ±å‘Šé é¢æ¸¬è©¦")
        test_path = paths[0]
        test_url = f"https://www.dlri.co.jp{test_path}"
        
        try:
            resp = requests.get(test_url, headers=HEADERS, timeout=5)
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æ¨™é¡Œ
            title_tag = soup.find('title')
            print(f"æ¨™é¡Œ: {title_tag.get_text(strip=True) if title_tag else 'N/A'}")
            
            # æ—¥æœŸ
            date_match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', resp.text)
            print(f"æ—¥æœŸ: {date_match.group(0) if date_match else 'N/A'}")
            
            # PDF
            pdf_tag = soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
            print(f"PDF: {pdf_tag['href'] if pdf_tag else 'N/A'}")
            
        except Exception as e:
            print(f"âœ— å ±å‘Šé é¢è¨ªå•å¤±æ•—: {e}")


if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    test_scraper()
    
    print("\n" + "=" * 60)
    print("æ­£å¼åŸ·è¡Œçˆ¬èŸ²")
    print("=" * 60)
    
    # åŸ·è¡Œçˆ¬èŸ²
    results = scrape()
    
    # é¡¯ç¤ºçµæœ
    if results:
        print(f"\nğŸ“„ æ‰¾åˆ° {len(results)} ç­†å ±å‘Š:")
        for i, r in enumerate(results, 1):
            print(f"\n{i}. [{r['Date']}] {r['Name']}")
            print(f"   ğŸ”— {r['Link']}")
