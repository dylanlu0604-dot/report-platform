import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€) - ğŸš€ é›™å¼•æ“çªç ´æ¨¡å¼...")
    reports = []
    seen_urls = set()
    found_paths = []

    # ==========================================
    # å¼•æ“ 1ï¼šæ”¹è‰¯ç‰ˆ MarsFlag API æ””æˆª (å½è£é€šè¡Œè­‰)
    # ==========================================
    api_url = "https://finder.api.mf.marsflag.com/api/v1/finder_service/documents/d3eff4d6/search"
    api_headers = HEADERS.copy()
    api_headers.update({
        'Accept': 'application/json',
        'Origin': 'https://www.dlri.co.jp',
        'Referer': 'https://www.dlri.co.jp/'
    })
    
    # MarsFlag API é€šå¸¸éœ€è¦ç‰¹å®šçš„æŸ¥è©¢åƒæ•¸ï¼Œæˆ‘å€‘æŠŠå¸¸è¦‹çš„éƒ½å¸¶ä¸Š
    queries = ["çµŒæ¸ˆ", "å¸‚å ´", "é‡‘è", "æ—¥æœ¬"]
    for q in queries:
        try:
            resp = requests.get(api_url, params={'match': q, 'q': q}, headers=api_headers, timeout=5)
            # æ¸…ç†è·³è„«å­—å…ƒå¾Œï¼Œç›´æ¥æš´åŠ›æŠ“å‡ºæ‰€æœ‰å ±å‘Šç¶²å€çš„å¾ŒåŠæ®µ
            text = resp.text.replace('\\/', '/')
            paths = re.findall(r'/report/[a-zA-Z0-9_/-]+\.html', text)
            found_paths.extend(paths)
        except:
            pass

    # ==========================================
    # å¼•æ“ 2ï¼šé¦–é  HTML æš´åŠ›æƒæ (å‚™ç”¨æ–¹æ¡ˆ)
    # ==========================================
    try:
        # å°±ç®— API æ“‹ä½æˆ‘å€‘ï¼Œé¦–é é€šå¸¸ä¹Ÿæœƒæ›ä¸Šæœ€æ–°å ±å‘Šçš„é€£çµ
        top_resp = requests.get("https://www.dlri.co.jp/", headers=HEADERS, timeout=5)
        paths = re.findall(r'/report/[a-zA-Z0-9_/-]+\.html', top_resp.text)
        found_paths.extend(paths)
    except:
        pass

    # ==========================================
    # ç¸½çµè™•ç†ï¼šé€ è¨ªæ¯å€‹æ‰¾åˆ°çš„å ±å‘Šå…§é æå–è³‡æ–™
    # ==========================================
    # å‰”é™¤é‡è¤‡çš„é€£çµ
    unique_paths = set(found_paths)
    print(f"  [åµæ¢å›å ±] ç¸½å…±æœé›†åˆ° {len(unique_paths)} å€‹æ½›åœ¨å ±å‘Šé€£çµï¼Œé–‹å§‹é€ä¸€æª¢é©—å…§é ...")
    
    for path in unique_paths:
        url = urljoin("https://www.dlri.co.jp", path)
        if url in seen_urls: continue
        seen_urls.add(url)
        
        try:
            # ç›´æ¥é€²å»å ±å‘Šçš„å°ˆå±¬é é¢
            detail_resp = requests.get(url, headers=HEADERS, timeout=5)
            detail_resp.encoding = 'utf-8'
            soup = BeautifulSoup(detail_resp.text, 'html.parser')
            
            # 1. å¾ <title> æŠ“æœ€ä¹¾æ·¨çš„æ¨™é¡Œ
            title_tag = soup.find('title')
            if not title_tag: continue
            title = title_tag.get_text(strip=True).split('|')[0].strip()
            
            # æ’é™¤éå ±å‘Šçš„ç¶²é 
            if len(title) < 5 or any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥"]): 
                continue
            
            # 2. ç›´æ¥åœ¨æ•´ä»½åŸå§‹ç¢¼è£¡é¢æ‰¾æ—¥æœŸ (æœ€ç„¡è…¦ä½†ä¹Ÿæœ€æœ‰æ•ˆ)
            date_text = None
            date_match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', detail_resp.text)
            if date_match:
                date_text = date_match.group(0)
                
            # å¦‚æœé€™ç¯‡å ±å‘Šæ²’æœ‰æ—¥æœŸï¼Œæˆ–æ˜¯è¶…é 30 å¤©ï¼Œå°±ç«‹åˆ»æ”¾æ£„
            if not date_text or not is_within_30_days(date_text):
                continue
                
            # 3. æ‰¾ PDF ä¸‹è¼‰é€£çµ
            pdf_tag = soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
            if not pdf_tag:
                pdf_tag = soup.find('a', string=re.compile(r'PDF', re.IGNORECASE))
                
            final_pdf = urljoin(url, pdf_tag['href']) if (pdf_tag and pdf_tag.get('href')) else url
                
            reports.append({
                "Source": "DLRI", 
                "Date": date_text, 
                "Name": title, 
                "Link": final_pdf
            })
            time.sleep(0.3) # ç¦®è²Œæ€§å»¶é²ï¼Œä¿è­·å°æ–¹ä¼ºæœå™¨
        except:
            pass

    print(f"  âœ… DLRI æœ€çµ‚æˆåŠŸæ”¶éŒ„ {len(reports)} ç­†å ±å‘Š")
    return reports
