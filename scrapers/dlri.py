import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from scrapers.utils import is_within_30_days, fetch_real_pdf_link

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½) - ğŸ­ Playwright çœŸäººæ¨¡æ“¬æ¨¡å¼...")
    reports = []
    
    # å‹•æ…‹è¼‰å…¥ Playwright
    try:
        from playwright.sync_api import sync_playwright
    except ImportError:
        print("  âŒ å°šæœªå®‰è£ Playwrightï¼Œè«‹ç¢ºèª requirements.txt")
        return reports

    # å•Ÿå‹•çœŸå¯¦çš„ç€è¦½å™¨å¼•æ“
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        # å½è£æˆä¸€èˆ¬çš„ Windows Chrome ç€è¦½å™¨
        context = browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        )
        page = context.new_page()
        
        try:
            # å‰å¾€å ±å‘Šç¸½è¦½é é¢
            page.goto("https://www.dlri.co.jp/report_index.html", wait_until="networkidle", timeout=20000)
            
            # ğŸŒŸ é—œéµï¼šç­‰å¾…ç¶²é ä¸Šçš„ JavaScript åŸ·è¡Œï¼Œç›´åˆ°å ±å‘Šçš„ <a> é€£çµå‡ºç¾ç‚ºæ­¢
            page.wait_for_selector(".list a", timeout=15000)
            
            # æŠ“å–æ¸²æŸ“å®Œæˆå¾Œçš„å®Œæ•´ HTML
            html = page.content()
            soup = BeautifulSoup(html, 'html.parser')
            
            # ä»¥ä¸‹æ¢å¾©æˆ‘å€‘ç†Ÿæ‚‰çš„ Beautifulsoup è§£æé‚è¼¯
            links = soup.find_all('a', href=re.compile(r'/report/'))
            seen_hrefs = set()
            
            for tag in links:
                href = tag.get('href')
                title = tag.get_text(strip=True)
                
                # æ’é™¤å¤ªçŸ­æˆ–é‡è¤‡çš„é›œè¨Š
                if len(title) < 5 or href in seen_hrefs: continue
                if any(kw in title for kw in ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›"]): continue
                
                # æ‰¾æ—¥æœŸ
                date_text = None
                parent = tag.find_parent()
                if parent:
                    match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', parent.get_text())
                    if match: date_text = match.group(0)
                    
                # åš´æ ¼æŠŠé—œï¼šæ²’æ—¥æœŸæˆ–è¶…é30å¤©å°±è¸¢æ‰
                if not date_text or not is_within_30_days(date_text): continue
                
                seen_hrefs.add(href)
                link = urljoin("https://www.dlri.co.jp", href)
                
                # è‹¥å…§é ä¹Ÿè¢« CloudFront æ“‹ï¼Œå°±ç›´æ¥çµ¦ç¶²é é€£çµ
                final_pdf = fetch_real_pdf_link(link)
                
                reports.append({
                    "Source": "DLRI", 
                    "Date": date_text, 
                    "Name": title, 
                    "Link": final_pdf
                })
                
        except Exception as e:
            print(f"  âŒ Playwright åŸ·è¡Œå¤±æ•—: {e}")
        finally:
            browser.close()
            
    print(f"  âœ… DLRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
