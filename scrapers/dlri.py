import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days, fetch_real_pdf_link

def scrape():
    """
    æ”¹é€²ç‰ˆ DLRI çˆ¬èŸ²
    
    ä¸»è¦æ”¹é€²:
    1. æ“´å¤§æ—¥æœŸæœå°‹ç¯„åœ
    2. æ›´å¯¬é¬†çš„é€£çµéæ¿¾æ¢ä»¶
    3. æ›´å¤šçš„æ—¥æœŸæ ¼å¼æ”¯æ´
    4. æ›´å¥½çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒ
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€)...")
    base_url = "https://www.dlri.co.jp"
    target_url = "https://www.dlri.co.jp/report_index.html"
    reports = []
    
    try:
        resp = requests.get(target_url, headers=HEADERS, timeout=15)
        resp.raise_for_status()  # æª¢æŸ¥ HTTP éŒ¯èª¤
        soup = BeautifulSoup(resp.content, 'html.parser')
        
        # æ‰¾æ‰€æœ‰é€£çµ
        links = soup.find_all('a', href=True)
        seen_hrefs = set()
        
        # ç”¨æ–¼é™¤éŒ¯
        total_links = len(links)
        report_candidates = 0
        date_found = 0
        within_30_days = 0
        
        for tag in links:
            href = tag['href']
            title = tag.get_text(strip=True)
            
            # 1. åŸºæœ¬éæ¿¾
            if len(title) < 5 or href in seen_hrefs:
                continue
            
            # 2. å¿…é ˆåŒ…å« /report/ ä¸”ä»¥ .html çµå°¾
            if "/report/" not in href or not href.endswith('.html'):
                continue
            
            report_candidates += 1
            
            # 3. æ’é™¤æ˜é¡¯çš„å°èˆªé€£çµ
            exclude_keywords = ["ä¸€è¦§", "List", "åŸ·ç­†è€…", "åˆ†é‡åˆ¥", "ãŠçŸ¥ã‚‰ã›", "æ¤œç´¢", "ãƒãƒƒã‚¯ãƒŠãƒ³ãƒãƒ¼"]
            if any(kw in title for kw in exclude_keywords):
                continue
            
            # 4. æ“´å¤§æ—¥æœŸæœå°‹ç¯„åœ - é€™æ˜¯é—œéµæ”¹é€²!
            date_text = None
            search_contexts = []
            
            # æ–¹æ³• A: æª¢æŸ¥é€£çµçš„ç›´æ¥çˆ¶å…ƒç´ 
            parent = tag.find_parent()
            if parent:
                search_contexts.append(parent.get_text())
                
                # æª¢æŸ¥å‰ä¸€å€‹å…„å¼Ÿå…ƒç´  (æ—¥æœŸå¯èƒ½åœ¨é€£çµå‰é¢)
                prev = parent.find_previous_sibling()
                if prev:
                    search_contexts.append(prev.get_text())
                
                # æª¢æŸ¥å¾Œä¸€å€‹å…„å¼Ÿå…ƒç´ 
                next_sib = parent.find_next_sibling()
                if next_sib:
                    search_contexts.append(next_sib.get_text())
            
            # æ–¹æ³• B: æª¢æŸ¥åŒ…å«æ­¤é€£çµçš„æ•´å€‹å®¹å™¨ (li, div, tr, article)
            container = tag.find_parent(['li', 'div', 'tr', 'article', 'section', 'td'])
            if container:
                search_contexts.append(container.get_text())
            
            # æ–¹æ³• C: æª¢æŸ¥é€£çµæ–‡å­—æœ¬èº« (æœ‰æ™‚æ—¥æœŸå°±åœ¨æ¨™é¡Œè£¡)
            search_contexts.append(title)
            
            # åˆä½µæ‰€æœ‰ä¸Šä¸‹æ–‡
            full_context = " ".join(search_contexts)
            
            # æ”¯æ´å¤šç¨®æ—¥æœŸæ ¼å¼
            date_patterns = [
                r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}[æ—¥]?',  # 2026.02.16, 2026å¹´2æœˆ16æ—¥
                r'20\d{2}[-]\d{1,2}[-]\d{1,2}',             # 2026-02-16
                r'20\d{2}/\d{1,2}/\d{1,2}',                 # 2026/2/16
            ]
            
            for pattern in date_patterns:
                match = re.search(pattern, full_context)
                if match:
                    date_text = match.group(0)
                    date_found += 1
                    break
            
            # 5. æ‰¾ä¸åˆ°æ—¥æœŸå°±è·³é
            if not date_text:
                continue
            
            # 6. æª¢æŸ¥æ˜¯å¦åœ¨30å¤©å…§
            if not is_within_30_days(date_text):
                continue
            
            within_30_days += 1
            
            # 7. åŠ å…¥æ¸…å–®
            seen_hrefs.add(href)
            full_link = urljoin(base_url, href)
            
            # å˜—è©¦æ‰¾åˆ°å¯¦éš›çš„ PDF é€£çµ
            try:
                final_pdf = fetch_real_pdf_link(full_link)
            except Exception as e:
                print(f"    âš ï¸  ç„¡æ³•å–å¾— PDF: {e}")
                final_pdf = full_link
            
            reports.append({
                "Source": "DLRI",
                "Date": date_text,
                "Name": title,
                "Link": final_pdf
            })
        
        # é™¤éŒ¯è³‡è¨Š
        print(f"    ğŸ“Š ç¸½é€£çµæ•¸: {total_links}")
        print(f"    ğŸ“Š å ±å‘Šå€™é¸: {report_candidates}")
        print(f"    ğŸ“Š æ‰¾åˆ°æ—¥æœŸ: {date_found}")
        print(f"    ğŸ“Š 30å¤©å…§: {within_30_days}")
        
    except requests.RequestException as e:
        print(f"  âŒ ç¶²è·¯éŒ¯èª¤: {e}")
    except Exception as e:
        print(f"  âŒ DLRI å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"  âœ… DLRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports


def scrape_with_alternative_strategy():
    """
    æ›¿ä»£ç­–ç•¥:å¦‚æœä¸»è¦æ–¹æ³•å¤±æ•—,å˜—è©¦ä¸åŒçš„æ–¹æ³•
    
    å¯èƒ½çš„å•é¡Œ:
    1. ç¶²ç«™çµæ§‹æ”¹è®Š
    2. å ±å‘Šé é¢ URL æ”¹è®Š
    3. æ—¥æœŸæ ¼å¼æ”¹è®Š
    """
    print("\nğŸ”„ å˜—è©¦æ›¿ä»£ç­–ç•¥...")
    base_url = "https://www.dlri.co.jp"
    
    # å¯èƒ½çš„å ±å‘Šæ¸…å–®é é¢
    alternative_urls = [
        "https://www.dlri.co.jp/report_index.html",
        "https://www.dlri.co.jp/report.html",
        "https://www.dlri.co.jp/report/index.html",
        "https://www.dlri.co.jp/research/index.html",
    ]
    
    for url in alternative_urls:
        try:
            print(f"  å˜—è©¦: {url}")
            resp = requests.get(url, headers=HEADERS, timeout=10)
            
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.content, 'html.parser')
                
                # åˆ†æé é¢çµæ§‹
                report_links = soup.find_all('a', href=re.compile(r'/report/.*\.html'))
                print(f"    âœ“ æ‰¾åˆ° {len(report_links)} å€‹å ±å‘Šé€£çµ")
                
                # é¡¯ç¤ºå‰å¹¾å€‹æ¨£æœ¬
                for i, link in enumerate(report_links[:3]):
                    print(f"      {i+1}. {link.get_text(strip=True)[:50]}")
                
                return soup  # è¿”å›æˆåŠŸçš„é é¢ä¾›é€²ä¸€æ­¥åˆ†æ
                
        except Exception as e:
            print(f"    âœ— å¤±æ•—: {e}")
    
    return None


# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤è…³æœ¬
if __name__ == "__main__":
    reports = scrape()
    
    if reports:
        print("\nğŸ“„ æ‰¾åˆ°çš„å ±å‘Š:")
        for i, r in enumerate(reports, 1):
            print(f"{i}. [{r['Date']}] {r['Name']}")
            print(f"   {r['Link']}\n")
    else:
        print("\nâ“ æ²’æœ‰æ‰¾åˆ°å ±å‘Šã€‚å¯èƒ½çš„åŸå› :")
        print("   1. ç¶²ç«™æœ€è¿‘ 30 å¤©å…§æ²’æœ‰æ–°å ±å‘Š")
        print("   2. ç¶²ç«™çµæ§‹æ”¹è®Š,æ—¥æœŸè§£æå¤±æ•—")
        print("   3. ç¶²è·¯é€£æ¥å•é¡Œ")
        print("\n   å»ºè­°:")
        print("   - æª¢æŸ¥ is_within_30_days() å‡½æ•¸æ˜¯å¦æ­£å¸¸å·¥ä½œ")
        print("   - æ‰‹å‹•è¨ªå•ç¶²ç«™ç¢ºèªæœ€è¿‘æ˜¯å¦æœ‰æ–°å ±å‘Š")
        print("   - æª¢æŸ¥ç¶²ç«™ HTML çµæ§‹æ˜¯å¦æ”¹è®Š")
