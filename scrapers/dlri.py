import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– DLRI (ç¬¬ä¸€ç”Ÿå‘½ç¶“æ¿Ÿç ”ç©¶æ‰€) - ğŸš€ API æ””æˆªæ¨¡å¼...")
    # ç ´è§£æ³•ï¼šç›´æ¥å‘ MarsFlag API çš„åº•å±¤è³‡æ–™åº«è¦è³‡æ–™ï¼
    api_url = "https://finder.api.mf.marsflag.com/api/v1/finder_service/documents/d3eff4d6/search"
    
    # ä¸Ÿå¹¾å€‹æ³›ç”¨é—œéµå­—çµ¦ APIï¼Œä¿è­‰èƒ½æŠŠæœ€æ–°å ±å‘Šéƒ½æŒ–å‡ºä¾†
    queries = ["", "çµŒæ¸ˆ", "æ—¥æœ¬", "å¸‚å ´"]
    reports = []
    seen_urls = set()
    
    for q in queries:
        try:
            resp = requests.get(api_url, params={'q': q, 'limit': 30}, headers=HEADERS, timeout=10)
            
            # API å‚³å›çš„ JSON è£¡é¢çš„ç¶²å€å¯èƒ½æœƒè¢«è·³è„« (\/)ï¼Œå…ˆé‚„åŸå®ƒ
            text = resp.text.replace('\\/', '/')
            
            # åˆ©ç”¨æ­£è¦è¡¨é”å¼ï¼Œæš´åŠ›æŠ“å–æ‰€æœ‰ç¬¦åˆå ±å‘Šæ ¼å¼çš„ç¶²å€ (ç„¡è¦–æœªçŸ¥çš„ JSON çµæ§‹)
            urls = re.findall(r'https?://www\.dlri\.co\.jp/report/[a-zA-Z0-9_/-]+\.html', text)
            
            for url in urls:
                if url in seen_urls: continue
                seen_urls.add(url)
                
                # æœ‰äº†ç¶²å€å¾Œï¼Œç›´æ¥è¨ªå•è©²å ±å‘Šçš„ã€Œéœæ…‹å°ˆå±¬å…§é ã€æŠ“å–æ¨™é¡Œå’Œæ—¥æœŸ
                try:
                    detail_resp = requests.get(url, headers=HEADERS, timeout=5)
                    detail_resp.encoding = 'utf-8'
                    soup = BeautifulSoup(detail_resp.text, 'html.parser')
                    
                    # 1. æŠ“æ¨™é¡Œ (å»é™¤ "| ç¬¬ä¸€ç”Ÿå‘½..." å¾Œç¶´)
                    title_tag = soup.find('title')
                    if not title_tag: continue
                    title = title_tag.get_text(strip=True).split('|')[0].strip()
                    if len(title) < 5 or "ä¸€è¦§" in title: continue
                    
                    # 2. æŠ“æ—¥æœŸ
                    date_text = None
                    date_match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}', detail_resp.text)
                    if date_match:
                        date_text = date_match.group(0)
                        
                    # åš´æ ¼å®ˆé–€å“¡ï¼šæ²’æ‰¾åˆ°æ—¥æœŸæˆ–è¶…é30å¤©ï¼Œç›´æ¥è¸¢é™¤
                    if not date_text or not is_within_30_days(date_text):
                        continue
                        
                    # 3. æŠ“ PDF é€£çµ (ç›´æ¥åœ¨å…§é æ‰¾ï¼Œçœå»ä¸€æ¬¡é¡å¤–è«‹æ±‚æ™‚é–“)
                    pdf_tag = soup.find('a', href=re.compile(r'\.pdf$', re.IGNORECASE))
                    if not pdf_tag:
                        pdf_tag = soup.find('a', string=re.compile(r'PDF', re.IGNORECASE))
                        
                    if pdf_tag and pdf_tag.get('href'):
                        final_pdf = urljoin(url, pdf_tag['href'])
                    else:
                        final_pdf = url
                        
                    reports.append({
                        "Source": "DLRI", 
                        "Date": date_text, 
                        "Name": title, 
                        "Link": final_pdf
                    })
                    time.sleep(0.3) # ç¦®è²Œæ€§å»¶é²
                except:
                    pass
        except Exception as e:
            print(f"  âŒ API æŸ¥è©¢å¤±æ•—: {e}")
            
    print(f"  âœ… DLRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
