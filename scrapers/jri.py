import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from scrapers.utils import HEADERS, is_within_30_days, fetch_real_pdf_link

def scrape():
    print("ğŸ” æ­£åœ¨çˆ¬å– JRI (Year List)...")
    base_url = "https://www.jri.co.jp"
    target_url = "https://www.jri.co.jp/report/year/"
    reports = []
    
    try:
        resp = requests.get(target_url, headers=HEADERS)
        soup = BeautifulSoup(resp.content, 'html.parser')
        links = soup.find_all('a', href=True)
        
        for tag in links:
            href = tag['href']
            title = tag.get_text(strip=True)
            
            # 1. åŸºæœ¬æ’é™¤ï¼šå¤ªçŸ­ã€éå ±å‘Šçš„é—œéµå­—
            if len(title) < 5: continue
            if any(kw in title for kw in ["ä¸€è¦§", "List", "YouTube", "Twitter", "æ¡ç”¨", "ãŠçŸ¥ã‚‰ã›", "Column", "ã‚³ãƒ©ãƒ "]): continue
            if href.startswith('#') or href.endswith('/'): continue
            
            # 2. åš´æ ¼å°‹æ‰¾æ—¥æœŸ
            date_text = None
            parent = tag.find_parent()
            if parent:
                # æ“´å¤§æœå°‹ç¯„åœï¼šæ‰¾å°‹çˆ¶ç¯€é»èˆ‡å‰ä¸€å€‹ç¯€é»çš„æ–‡å­—
                txt = parent.get_text()
                prev_sibling = parent.find_previous_sibling()
                if prev_sibling:
                    txt += " " + prev_sibling.get_text()
                    
                # åŒ¹é…ç²¾ç¢ºæ ¼å¼ï¼š2026å¹´02æœˆ06æ—¥ æˆ– 2026.02.06
                match = re.search(r'20\d{2}[./å¹´]\d{1,2}[./æœˆ]\d{1,2}æ—¥?', txt)
                if match: 
                    date_text = match.group(0)

            # 3. æ ¸å¿ƒä¿®æ­£ï¼šå¦‚æœã€Œæ‰¾ä¸åˆ°æ—¥æœŸã€ï¼Œä»£è¡¨å®ƒçµ•å°ä¸æ˜¯å ±å‘Šï¼Œç›´æ¥è·³éï¼
            if not date_text:
                continue
                
            # 4. æ—¥æœŸå€é–“æª¢æŸ¥ï¼ˆè¶…é30å¤©å°±éæ¿¾æ‰ï¼‰
            if not is_within_30_days(date_text):
                continue

            # 5. ç¢ºä¿æ˜¯å ±å‘Šçš„å°ˆå±¬é€£çµæ ¼å¼ (æ’é™¤ä¸€èˆ¬ç¶²é )
            if "page.jsp?id=" not in href and ".pdf" not in href.lower():
                continue

            link = urljoin(base_url, href)
            final_pdf = fetch_real_pdf_link(link)
            reports.append({"Source": "JRI", "Date": date_text, "Name": title, "Link": final_pdf})
                
    except Exception as e:
        print(f"  âŒ JRI å¤±æ•—: {e}")
    
    print(f"  âœ… JRI æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports
