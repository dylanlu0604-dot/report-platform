import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, unquote
import time
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–çˆ¬èŸ² (è¶…ç´šéˆæ´»ç‰ˆ)
    
    ç­–ç•¥:
    1. ç›´æ¥æ‰¾ PDF é€£çµ (ä»»ä½•ä½ç½®)
    2. æ‰¾æ‰€æœ‰å¯èƒ½æ˜¯å ±å‘Šçš„é€£çµ
    3. æœå°‹é é¢ä¸­çš„ data å±¬æ€§å’Œ JSON
    4. æª¢æŸ¥æ˜¯å¦æœ‰ API è¼‰å…¥
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– CTBC (ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–)...")
    reports = []
    seen_urls = set()
    
    base_url = "https://www.ctbcbank.com"
    target_url = "https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html"
    
    try:
        # ç²å–é é¢
        resp = requests.get(target_url, headers=HEADERS, timeout=15)
        resp.encoding = 'utf-8'
        soup = BeautifulSoup(resp.text, 'html.parser')
        
        print(f"  ğŸ“„ é é¢å¤§å°: {len(resp.text):,} å­—å…ƒ")
        
        # ==========================================
        # ç­–ç•¥ 1: è¶…ç´šå¯¬é¬†çš„ PDF æœå°‹
        # ==========================================
        print("  [ç­–ç•¥ 1] å»£åŸŸ PDF æœå°‹...")
        
        # æ–¹æ³• A: æ­£å‰‡æœå°‹æ‰€æœ‰ PDF URL (åŒ…æ‹¬ JavaScript ä¸­çš„)
        pdf_urls_raw = re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', resp.text, re.IGNORECASE)
        pdf_urls_raw.extend(re.findall(r'href=["\']([^"\']*\.pdf[^"\']*)["\']', resp.text, re.IGNORECASE))
        pdf_urls_raw.extend(re.findall(r'(https?://[^\s<>"\']+\.pdf)', resp.text, re.IGNORECASE))
        
        # æ–¹æ³• B: BeautifulSoup æ‰¾æ‰€æœ‰ <a> æ¨™ç±¤
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if '.pdf' in href.lower():
                pdf_urls_raw.append(href)
        
        # æ–¹æ³• C: æª¢æŸ¥ data å±¬æ€§
        for elem in soup.find_all(attrs={'data-url': True}):
            url = elem.get('data-url', '')
            if '.pdf' in url.lower():
                pdf_urls_raw.append(url)
        
        for elem in soup.find_all(attrs={'data-href': True}):
            url = elem.get('data-href', '')
            if '.pdf' in url.lower():
                pdf_urls_raw.append(url)
        
        # å»é‡å’Œæ¸…ç†
        pdf_urls = []
        for url in set(pdf_urls_raw):
            # æ¸…ç† URL
            url = url.strip().strip('"').strip("'")
            if url and '.pdf' in url.lower():
                full_url = urljoin(base_url, url)
                if full_url not in seen_urls:
                    pdf_urls.append(full_url)
                    seen_urls.add(full_url)
        
        print(f"    æ‰¾åˆ° {len(pdf_urls)} å€‹ PDF URL")
        
        # è™•ç†æ‰¾åˆ°çš„ PDF
        for pdf_url in pdf_urls:
            # å˜—è©¦å¾ URL æå–è³‡è¨Š
            title, date_text = extract_info_from_url(pdf_url)
            
            # å¦‚æœ URL æ²’è³‡è¨Š,å˜—è©¦æ‰¾å°æ‡‰çš„ <a> æ¨™ç±¤
            if not title or not date_text:
                link = soup.find('a', href=lambda x: x and pdf_url in urljoin(base_url, x))
                if link:
                    if not title:
                        title = extract_title_from_link(link)
                    if not date_text:
                        date_text = extract_date_from_link(link)
            
            # å¦‚æœé‚„æ˜¯æ²’æ¨™é¡Œ,ç”¨æª”å
            if not title or len(title) < 5:
                filename = unquote(pdf_url.split('/')[-1].replace('.pdf', '').replace('.PDF', ''))
                title = filename
            
            # æ²’æ—¥æœŸå°±è·³é
            if not date_text:
                continue
            
            # æª¢æŸ¥ 30 å¤©å…§
            if not is_within_30_days(date_text):
                continue
            
            # æ¸…ç†æ¨™é¡Œ
            title = clean_title(title, date_text)
            
            reports.append({
                "Source": "CTBC",
                "Date": date_text,
                "Name": title,
                "Link": pdf_url
            })
        
        # ==========================================
        # ç­–ç•¥ 2: å¦‚æœæ²’æ‰¾åˆ°,æœå°‹æ‰€æœ‰å¯èƒ½çš„å ±å‘Šé€£çµ
        # ==========================================
        if len(reports) == 0:
            print("  [ç­–ç•¥ 2] æœå°‹å ±å‘Šé€£çµ...")
            
            # è¶…ç´šå¯¬é¬†çš„é—œéµå­—
            keywords = [
                'å ±å‘Š', 'è©•è«–', 'åˆ†æ', 'å¸‚å ´', 'å±•æœ›', 'è§€é»',
                'report', 'analysis', 'market', 'comment', 'review',
                'æœˆå ±', 'é€±å ±', 'æ—¥å ±', 'å°ˆé¡Œ', 'ç ”ç©¶'
            ]
            
            report_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                # æ’é™¤æ˜é¡¯ä¸ç›¸é—œçš„
                if any(exclude in href.lower() for exclude in ['javascript:', 'mailto:', '#']):
                    continue
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«é—œéµå­—
                if any(kw in text or kw in href for kw in keywords):
                    full_url = urljoin(base_url, href)
                    if 'ctbcbank.com' in full_url and full_url not in seen_urls:
                        report_links.append((full_url, text))
                        seen_urls.add(full_url)
            
            print(f"    æ‰¾åˆ° {len(report_links)} å€‹å¯èƒ½çš„å ±å‘Šé€£çµ")
            
            # è¨ªå•æ¯å€‹é€£çµæ‰¾ PDF
            for detail_url, preview_title in report_links[:30]:
                try:
                    detail_resp = requests.get(detail_url, headers=HEADERS, timeout=10)
                    detail_resp.encoding = 'utf-8'
                    detail_soup = BeautifulSoup(detail_resp.text, 'html.parser')
                    
                    # åœ¨è©³æƒ…é æ‰¾ PDF
                    pdf_link = detail_soup.find('a', href=re.compile(r'\.pdf$', re.I))
                    
                    if not pdf_link:
                        # å˜—è©¦æ‰¾åŒ…å« PDF çš„ JavaScript
                        pdf_urls_in_page = re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', 
                                                       detail_resp.text, re.IGNORECASE)
                        if pdf_urls_in_page:
                            pdf_url = urljoin(detail_url, pdf_urls_in_page[0])
                        else:
                            continue
                    else:
                        pdf_url = urljoin(detail_url, pdf_link['href'])
                    
                    if pdf_url in seen_urls:
                        continue
                    seen_urls.add(pdf_url)
                    
                    # æ¨™é¡Œ
                    h1 = detail_soup.find('h1')
                    title = h1.get_text(strip=True) if h1 else preview_title
                    
                    # æ—¥æœŸ
                    date_text = extract_date_from_text(detail_resp.text)
                    
                    if not date_text or not is_within_30_days(date_text):
                        continue
                    
                    title = clean_title(title, date_text)
                    
                    reports.append({
                        "Source": "CTBC",
                        "Date": date_text,
                        "Name": title,
                        "Link": pdf_url
                    })
                    
                    time.sleep(0.3)
                    
                except Exception:
                    continue
        
        # ==========================================
        # ç­–ç•¥ 3: æª¢æŸ¥ JSON æˆ– data å±¬æ€§
        # ==========================================
        if len(reports) == 0:
            print("  [ç­–ç•¥ 3] æœå°‹ JSON è³‡æ–™...")
            
            # åœ¨ script æ¨™ç±¤ä¸­æ‰¾ JSON
            for script in soup.find_all('script'):
                if script.string:
                    # å°‹æ‰¾çœ‹èµ·ä¾†åƒå ±å‘Šè³‡æ–™çš„ JSON
                    json_matches = re.findall(r'\{[^}]*(?:pdf|report|title|date)[^}]*\}', 
                                             script.string, re.IGNORECASE)
                    for json_str in json_matches:
                        try:
                            import json
                            data = json.loads(json_str)
                            # å˜—è©¦æå– PDF URL
                            # é€™è£¡å¯ä»¥æ ¹æ“šå¯¦éš› JSON çµæ§‹èª¿æ•´
                        except:
                            pass
        
    except Exception as e:
        print(f"  âŒ CTBC çˆ¬å–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    # å¦‚æœå®Œå…¨æ²’æ‰¾åˆ°,çµ¦å‡ºè¨ºæ–·å»ºè­°
    if len(reports) == 0:
        print("\n  âš ï¸  æœªæ‰¾åˆ°ä»»ä½•å ±å‘Š,å¯èƒ½åŸå› :")
        print("     1. ç¶²ç«™ä½¿ç”¨ JavaScript å‹•æ…‹è¼‰å…¥ (éœ€è¦ Selenium)")
        print("     2. å…§å®¹åœ¨ iframe ä¸­")
        print("     3. éœ€è¦ç™»å…¥æ‰èƒ½è¨ªå•")
        print("     4. ç¶²ç«™çµæ§‹å®Œå…¨ä¸åŒ")
        print("\n  ğŸ’¡ å»ºè­°:")
        print("     1. æ‰‹å‹•è¨ªå•ç¶²ç«™ç¢ºèªå ±å‘Šä½ç½®")
        print("     2. ä½¿ç”¨ç€è¦½å™¨é–‹ç™¼è€…å·¥å…·æŸ¥çœ‹ Network è«‹æ±‚")
        print("     3. é‹è¡Œè¨ºæ–·è…³æœ¬: python diagnose_ctbc_live.py")
    
    print(f"  âœ… CTBC æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports


def extract_info_from_url(url):
    """å¾ URL æå–æ¨™é¡Œå’Œæ—¥æœŸ"""
    title = None
    date_text = None
    
    # URL decode
    url_decoded = unquote(url)
    
    # æå–æª”å
    filename = url_decoded.split('/')[-1].replace('.pdf', '').replace('.PDF', '')
    
    # å¾æª”åæå–æ—¥æœŸ
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}[_-]\d{1,2}[_-]\d{1,2}',
        r'20\d{2}\d{2}\d{2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_text = match.group(0)
            # æ¨™æº–åŒ–æ—¥æœŸæ ¼å¼
            if len(date_text) == 8 and date_text.isdigit():
                date_text = f"{date_text[:4]}/{date_text[4:6]}/{date_text[6:8]}"
            break
    
    # å¾æª”åæå–æ¨™é¡Œ (ç§»é™¤æ—¥æœŸéƒ¨åˆ†)
    if date_text:
        title = re.sub(r'20\d{2}[å¹´_\-/]\d{1,2}[æœˆ_\-/]\d{1,2}[æ—¥]?', '', filename)
        title = re.sub(r'20\d{2}\d{2}\d{2}', '', title)
    else:
        title = filename
    
    return title, date_text


def extract_title_from_link(link):
    """å¾é€£çµå…ƒç´ æå–æ¨™é¡Œ"""
    # ç­–ç•¥ 1: é€£çµæ–‡å­—
    title = link.get_text(strip=True)
    
    # ç­–ç•¥ 2: title å±¬æ€§
    if not title or len(title) < 5:
        title = link.get('title', '')
    
    # ç­–ç•¥ 3: çˆ¶å…ƒç´ 
    if not title or len(title) < 5:
        parent = link.find_parent(['li', 'div', 'td'])
        if parent:
            title = re.sub(r'\s+', ' ', parent.get_text(strip=True))
    
    return title


def extract_date_from_link(link):
    """å¾é€£çµå‘¨åœæå–æ—¥æœŸ"""
    date_text = None
    
    parent = link.find_parent(['li', 'div', 'tr', 'td', 'article'])
    if parent:
        search_text = parent.get_text()
    else:
        search_text = link.get_text()
    
    date_text = extract_date_from_text(search_text)
    return date_text


def extract_date_from_text(text):
    """å¾æ–‡å­—ä¸­æå–æ—¥æœŸ"""
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}/\d{1,2}/\d{1,2}',
        r'20\d{2}\.\d{1,2}\.\d{1,2}',
        r'20\d{2}-\d{1,2}-\d{1,2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    
    return None


def clean_title(title, date_text):
    """æ¸…ç†æ¨™é¡Œ"""
    # ç§»é™¤æ—¥æœŸ
    title = re.sub(r'20\d{2}[å¹´/.\-_]\d{1,2}[æœˆ/.\-_]\d{1,2}[æ—¥]?', '', title)
    
    # ç§»é™¤ PDF å­—æ¨£
    title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\(PDF\)', '', title, flags=re.IGNORECASE)
    
    # ç§»é™¤å¤šé¤˜ç©ºç™½å’Œç¬¦è™Ÿ
    title = re.sub(r'[_\-]+', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    
    return title


if __name__ == "__main__":
    results = scrape()
    
    if results:
        print(f"\nğŸ“„ æ‰¾åˆ°çš„å ±å‘Š:")
        for i, r in enumerate(results, 1):
            print(f"\n{i}. [{r['Date']}] {r['Name'][:60]}")
            print(f"   ğŸ”— {r['Link']}")
