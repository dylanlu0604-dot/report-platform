import re
import time
from urllib.parse import urljoin, unquote
from bs4 import BeautifulSoup
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–çˆ¬èŸ² (å®Œå…¨å®¹éŒ¯ç‰ˆ)
    
    ç­–ç•¥:
    1. å…ˆæ¸¬è©¦ç¶²è·¯é€£é€šæ€§
    2. å¦‚æœç„¡æ³•é€£æ¥,å„ªé›…åœ°è¿”å›ç©ºåˆ—è¡¨
    3. ä¸å½±éŸ¿å…¶ä»–çˆ¬èŸ²çš„åŸ·è¡Œ
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– CTBC (ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–)...")
    reports = []
    
    base_url = "https://www.ctbcbank.com"
    target_url = "https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html"
    
    # å¿«é€Ÿç¶²è·¯æ¸¬è©¦ (5ç§’è¶…æ™‚)
    if not test_connectivity(base_url):
        print("  âš ï¸  ç„¡æ³•é€£æ¥åˆ°ä¸­åœ‹ä¿¡è¨—ç¶²ç«™")
        print("  ğŸ’¡ å¯èƒ½åŸå› :")
        print("     - GitHub Actions ç’°å¢ƒç¶²è·¯é™åˆ¶")
        print("     - ç¶²ç«™å°é– GitHub IP")
        print("     - ç¶²ç«™ç¶­è­·ä¸­")
        print("  â„¹ï¸  è·³éæ­¤çˆ¬èŸ²,ç¹¼çºŒåŸ·è¡Œå…¶ä»–çˆ¬èŸ²...")
        return reports
    
    # å˜—è©¦ä½¿ç”¨ Playwright
    html_content = None
    try:
        from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError, Error as PlaywrightError
        
        for attempt in range(2):  # æ¸›å°‘åˆ° 2 æ¬¡é‡è©¦ç¯€çœæ™‚é–“
            try:
                html_content = scrape_with_playwright(target_url, attempt + 1)
                if html_content:
                    break
            except Exception as e:
                if attempt < 1:
                    time.sleep(3)
    except ImportError:
        print("  âš ï¸  Playwright æœªå®‰è£,ä½¿ç”¨ requests...")
    
    # é™ç´šä½¿ç”¨ requests
    if not html_content:
        print("  ğŸ”„ ä½¿ç”¨ requests...")
        try:
            import requests
            resp = requests.get(target_url, headers=HEADERS, timeout=15)
            html_content = resp.text
            print(f"  âœ“ æˆåŠŸ ({len(html_content):,} å­—å…ƒ)")
        except Exception as e:
            print(f"  âŒ é€£æ¥å¤±æ•—: {type(e).__name__}")
            print("  â„¹ï¸  æ­¤çˆ¬èŸ²æš«æ™‚ç„¡æ³•ä½¿ç”¨")
            return reports
    
    # è§£æ HTML
    try:
        reports = parse_html(html_content, base_url)
    except Exception as e:
        print(f"  âŒ è§£æå¤±æ•—: {e}")
    
    print(f"  âœ… CTBC æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports


def test_connectivity(base_url, timeout=5):
    """å¿«é€Ÿæ¸¬è©¦ç¶²ç«™é€£é€šæ€§"""
    try:
        import requests
        resp = requests.head(base_url, timeout=timeout, allow_redirects=True)
        return resp.status_code < 500
    except:
        return False


def scrape_with_playwright(url, attempt):
    """ä½¿ç”¨ Playwright æŠ“å– (ç°¡åŒ–ç‰ˆ)"""
    from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
    
    print(f"  ğŸŒ Playwright å˜—è©¦ {attempt}/2...")
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(
                headless=True,
                args=['--disable-dev-shm-usage', '--no-sandbox']
            )
            
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            )
            
            page = context.new_page()
            
            try:
                page.goto(url, wait_until="domcontentloaded", timeout=20000)  # ç¸®çŸ­åˆ° 20 ç§’
            except PlaywrightTimeoutError:
                pass
            
            time.sleep(3)  # ç¸®çŸ­ç­‰å¾…æ™‚é–“
            html_content = page.content()
            
            browser.close()
            return html_content
            
    except Exception:
        return None


def parse_html(html_content, base_url):
    """è§£æ HTML æå–å ±å‘Š"""
    reports = []
    soup = BeautifulSoup(html_content, 'html.parser')
    seen_urls = set()
    
    # å»£åŸŸæœå°‹ PDF
    pdf_urls_raw = []
    
    # æ­£å‰‡æœå°‹
    pdf_urls_raw.extend(re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE))
    pdf_urls_raw.extend(re.findall(r'(/IB/api/adapters/IB_Adapter/resource/report/[^"\']+)', html_content))
    
    # BeautifulSoup æœå°‹
    for link in soup.find_all('a', href=True):
        href = link.get('href', '')
        if '.pdf' in href.lower() or '/resource/report/' in href.lower():
            pdf_urls_raw.append(href)
    
    # æ¸…ç† URL
    target_urls = []
    for url in set(pdf_urls_raw):
        url = url.strip().strip('"').strip("'")
        if url and ('.pdf' in url.lower() or '/resource/report/' in url.lower()):
            full_url = urljoin(base_url, url)
            if full_url not in seen_urls:
                target_urls.append(full_url)
                seen_urls.add(full_url)
    
    # è™•ç†æ¯å€‹ URL
    for target_url in target_urls:
        title, date_text = extract_info_from_url(target_url)
        
        if not title or not date_text:
            link = soup.find('a', href=lambda x: x and target_url in urljoin(base_url, x))
            if link:
                if not title:
                    title = extract_title_from_link(link)
                if not date_text:
                    date_text = extract_date_from_link(link)
        
        if not title or len(title) < 5:
            filename = unquote(target_url.split('/')[-1].replace('.pdf', ''))
            title = filename
        
        if not date_text:
            continue
        
        if not is_within_30_days(date_text):
            continue
        
        title = clean_title(title, date_text)
        
        reports.append({
            "Source": "CTBC",
            "Date": date_text,
            "Name": title,
            "Link": target_url
        })
    
    return reports


def extract_info_from_url(url):
    """å¾ URL æå–æ¨™é¡Œå’Œæ—¥æœŸ"""
    title = None
    date_text = None
    
    url_decoded = unquote(url)
    filename = url_decoded.split('/')[-1].replace('.pdf', '')
    
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}[_-]\d{1,2}[_-]\d{1,2}',
        r'20\d{2}\d{2}\d{2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_text = match.group(0)
            if len(date_text) == 8 and date_text.isdigit():
                date_text = f"{date_text[:4]}/{date_text[4:6]}/{date_text[6:8]}"
            break
    
    if date_text:
        title = re.sub(r'20\d{2}[å¹´_\-/]\d{1,2}[æœˆ_\-/]\d{1,2}[æ—¥]?', '', filename)
        title = re.sub(r'20\d{2}\d{2}\d{2}', '', title)
        if re.match(r'^[-_A-Za-z0-9]+$', title.strip()):
            title = None
    else:
        title = filename
    
    return title, date_text


def extract_title_from_link(link):
    """å¾é€£çµå…ƒç´ æå–æ¨™é¡Œ"""
    title = link.get_text(strip=True)
    if not title or len(title) < 5:
        title = link.get('title', '')
    if not title or len(title) < 5:
        parent = link.find_parent(['li', 'div', 'td'])
        if parent:
            title = re.sub(r'\s+', ' ', parent.get_text(strip=True))
    return title


def extract_date_from_link(link):
    """å¾é€£çµå‘¨åœæå–æ—¥æœŸ"""
    parent = link.find_parent(['li', 'div', 'tr', 'td'])
    search_text = parent.get_text() if parent else link.get_text()
    return extract_date_from_text(search_text)


def extract_date_from_text(text):
    """å¾æ–‡å­—ä¸­æå–æ—¥æœŸ"""
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}/\d{1,2}/\d{1,2}',
        r'20\d{2}\.\d{1,2}\.\d{1,2}',
        r'20\d{2}-\d{1,2}-\d{1,2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    return None


def clean_title(title, date_text):
    """æ¸…ç†æ¨™é¡Œ"""
    title = re.sub(r'20\d{2}[å¹´/.\-_]\d{1,2}[æœˆ/.\-_]\d{1,2}[æ—¥]?', '', title)
    title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\(PDF\)', '', title, flags=re.IGNORECASE)
    title = re.sub(r'[_\-]+', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    return title


if __name__ == "__main__":
    results = scrape()
    if results:
        print(f"\nğŸ“„ æ‰¾åˆ°çš„å ±å‘Š:")
        for i, r in enumerate(results, 1):
            print(f"\n{i}. [{r['Date']}] {r['Name'][:60]}")
            print(f"   ğŸ”— {r['Link']}")
