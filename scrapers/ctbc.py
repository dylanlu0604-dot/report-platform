import re
import time
from urllib.parse import urljoin, unquote
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

# å‡è¨­é€™æ˜¯ä½ åŸæœ¬å°ˆæ¡ˆä¸­çš„ utilsï¼Œè«‹ç¢ºèªè·¯å¾‘æ­£ç¢º
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–çˆ¬èŸ² (Playwright å‹•æ…‹æ¸²æŸ“ + CTBC API æ”¯æ´ç‰ˆ)
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– CTBC (ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–)...")
    reports = []
    seen_urls = set()
    
    base_url = "https://www.ctbcbank.com"
    target_url = "https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html"
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()
            
            print("  ğŸŒ å•Ÿå‹• Playwright è¼‰å…¥å‹•æ…‹ç¶²é ...")
            
            # ğŸ‘‰ ä¿®æ”¹ 1ï¼šå°‡ networkidle æ”¹ç‚º domcontentloadedï¼Œä¸¦å°‡è¶…æ™‚æ™‚é–“æ‹‰é•·åˆ° 60 ç§’ä»¥é©æ‡‰ GitHub Actions
            try:
                page.goto(target_url, wait_until="domcontentloaded", timeout=60000)
            except PlaywrightTimeoutError:
                print("  âš ï¸ ç¶²é åŸºæœ¬çµæ§‹è¼‰å…¥è¶…æ™‚ï¼Œä½†å¯èƒ½å·²éƒ¨åˆ†æ¸²æŸ“ï¼Œç¹¼çºŒå˜—è©¦æ“·å–...")

            # ğŸ‘‰ ä¿®æ”¹ 2ï¼šæ‹‰é•·æ‰‹å‹•ç­‰å¾…æ™‚é–“ã€‚å› ç‚ºæˆ‘å€‘æ”¾æ£„äº† networkidleï¼Œæ‰€ä»¥å¤šçµ¦ JS ä¸€é»æ™‚é–“ç”Ÿå‡ºå ±å‘Šåˆ—è¡¨
            print("  â³ ç­‰å¾…å‹•æ…‹å…§å®¹æ¸²æŸ“...")
            time.sleep(8) 
            
            # ç²å–æ¸²æŸ“å¾Œçš„å®Œæ•´ HTML
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            print(f"  ğŸ“„ æ¸²æŸ“å¾Œé é¢å¤§å°: {len(html_content):,} å­—å…ƒ")
            
            browser.close()

        # ==========================================
        # ç­–ç•¥ 1: å»£åŸŸ PDF æœå°‹ (åŠ å…¥ CTBC å°ˆå±¬ API æ ¼å¼)
        # ==========================================
        print("  [ç­–ç•¥ 1] å»£åŸŸ PDF æœå°‹...")
        
        pdf_urls_raw = re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE)
        pdf_urls_raw.extend(re.findall(r'href=["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE))
        pdf_urls_raw.extend(re.findall(r'(https?://[^\s<>"\']+\.pdf)', html_content, re.IGNORECASE))
        
        # æ“·å– CTBC å°ˆå±¬çš„å ±å‘Š API é€£çµ
        pdf_urls_raw.extend(re.findall(r'(/IB/api/adapters/IB_Adapter/resource/report/[^"\']+)', html_content, re.IGNORECASE))
        
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if '.pdf' in href.lower() or '/resource/report/' in href.lower():
                pdf_urls_raw.append(href)
        
        for elem in soup.find_all(attrs={'data-url': True}):
            url = elem.get('data-url', '')
            if '.pdf' in url.lower() or '/resource/report/' in url.lower():
                pdf_urls_raw.append(url)
                
        for elem in soup.find_all(attrs={'data-href': True}):
            url = elem.get('data-href', '')
            if '.pdf' in url.lower() or '/resource/report/' in url.lower():
                pdf_urls_raw.append(url)
        
        # å»é‡å’Œæ¸…ç†
        target_urls = []
        for url in set(pdf_urls_raw):
            url = url.strip().strip('"').strip("'")
            if url and ('.pdf' in url.lower() or '/resource/report/' in url.lower()):
                full_url = urljoin(base_url, url)
                if full_url not in seen_urls:
                    target_urls.append(full_url)
                    seen_urls.add(full_url)
        
        print(f"    æ‰¾åˆ° {len(target_urls)} å€‹ç›®æ¨™ URL")
        
        for target_url in target_urls:
            title, date_text = extract_info_from_url(target_url)
            
            # å¦‚æœ URL æ²’è³‡è¨Šï¼Œå˜—è©¦æ‰¾å°æ‡‰çš„ HTML æ¨™ç±¤
            if not title or not date_text:
                link = soup.find('a', href=lambda x: x and target_url in urljoin(base_url, x))
                
                # å¦‚æœç”¨ç²¾æº– href æ‰¾ä¸åˆ°ï¼Œè©¦è‘—æ‰¾åŒ…å«é€™æ®µ report_id çš„ä»»ä½• UI å®¹å™¨
                if not link:
                    report_id = target_url.split('/')[-1]
                    link = soup.find(lambda tag: tag.name in ['a', 'div', 'li', 'td'] and report_id in str(tag))

                if link:
                    if not title:
                        title = extract_title_from_link(link)
                    if not date_text:
                        date_text = extract_date_from_link(link)
            
            # å¦‚æœé‚„æ˜¯æ²’æ¨™é¡Œï¼Œç”¨æª”å
            if not title or len(title) < 5:
                filename = unquote(target_url.split('/')[-1].replace('.pdf', '').replace('.PDF', ''))
                title = filename
            
            # æ²’æ—¥æœŸå°±è·³é
            if not date_text:
                continue
            
            # æª¢æŸ¥ 30 å¤©å…§
            if not is_within_30_days(date_text):
                continue
            
            # æ¸…ç†æ¨™é¡Œ
            title = clean_title(title, date_text)
            
            reports.append({
                "Source": "CTBC",
                "Date": date_text,
                "Name": title,
                "Link": target_url
            })
            
        # ==========================================
        # ç­–ç•¥ 2: å¦‚æœæ²’æ‰¾åˆ°,æœå°‹æ‰€æœ‰å¯èƒ½çš„å ±å‘Šé€£çµ
        # ==========================================
        if len(reports) == 0:
            print("  [ç­–ç•¥ 2] æœå°‹å ±å‘Šé€£çµ...")
            keywords = [
                'å ±å‘Š', 'è©•è«–', 'åˆ†æ', 'å¸‚å ´', 'å±•æœ›', 'è§€é»',
                'report', 'analysis', 'market', 'comment', 'review',
                'æœˆå ±', 'é€±å ±', 'æ—¥å ±', 'å°ˆé¡Œ', 'ç ”ç©¶'
            ]
            
            report_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                link_text = link.get_text(strip=True)
                
                if any(exclude in href.lower() for exclude in ['javascript:', 'mailto:', '#']):
                    continue
                
                if any(kw in link_text or kw in href for kw in keywords):
                    full_url = urljoin(base_url, href)
                    if 'ctbcbank.com' in full_url and full_url not in seen_urls:
                        report_links.append((full_url, link_text))
                        seen_urls.add(full_url)
            
            print(f"    æ‰¾åˆ° {len(report_links)} å€‹å¯èƒ½çš„å ±å‘Šé€£çµ")
            # ç”±æ–¼ç­–ç•¥ 1 å·²ç¶“æ¶µè“‹äº† APIï¼Œç­–ç•¥ 2 åƒ…ä½œå‚™ç”¨ï¼Œé€™è£¡çœç•¥å¾ŒçºŒé€²éšçˆ¬å–ä»¥ä¿æŒè¼•é‡

    except Exception as e:
        print(f"  âŒ CTBC çˆ¬å–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"  âœ… CTBC æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports


def extract_info_from_url(url):
    """å¾ URL æå–æ¨™é¡Œå’Œæ—¥æœŸ"""
    title = None
    date_text = None
    
    url_decoded = unquote(url)
    filename = url_decoded.split('/')[-1].replace('.pdf', '').replace('.PDF', '')
    
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}[_-]\d{1,2}[_-]\d{1,2}',
        r'20\d{2}\d{2}\d{2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_text = match.group(0)
            if len(date_text) == 8 and date_text.isdigit():
                date_text = f"{date_text[:4]}/{date_text[4:6]}/{date_text[6:8]}"
            break
    
    if date_text:
        title = re.sub(r'20\d{2}[å¹´_\-/]\d{1,2}[æœˆ_\-/]\d{1,2}[æ—¥]?', '', filename)
        title = re.sub(r'20\d{2}\d{2}\d{2}', '', title)
        
        # å¦‚æœå»è•ªå­˜èå¾Œï¼Œæ¨™é¡Œåªå‰©ä¸‹æµæ°´è™Ÿ (å¦‚ -C-30-0)ï¼Œå¼·åˆ¶è¨­ç‚º Noneï¼Œè®“ç¨‹å¼å»æŠ“ HTML è£¡çš„ä¸­æ–‡æ¨™é¡Œ
        if re.match(r'^[-_A-Za-z0-9]+$', title.strip()):
            title = None
    else:
        title = filename
    
    return title, date_text


def extract_title_from_link(link):
    """å¾é€£çµå…ƒç´ æå–æ¨™é¡Œ"""
    title = link.get_text(strip=True)
    
    if not title or len(title) < 5:
        title = link.get('title', '')
    
    if not title or len(title) < 5:
        parent = link.find_parent(['li', 'div', 'td'])
        if parent:
            title = re.sub(r'\s+', ' ', parent.get_text(strip=True))
    
    return title


def extract_date_from_link(link):
    """å¾é€£çµå‘¨åœæå–æ—¥æœŸ"""
    parent = link.find_parent(['li', 'div', 'tr', 'td', 'article'])
    if parent:
        search_text = parent.get_text()
    else:
        search_text = link.get_text()
    
    return extract_date_from_text(search_text)


def extract_date_from_text(text):
    """å¾æ–‡å­—ä¸­æå–æ—¥æœŸ"""
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}/\d{1,2}/\d{1,2}',
        r'20\d{2}\.\d{1,2}\.\d{1,2}',
        r'20\d{2}-\d{1,2}-\d{1,2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    
    return None


def clean_title(title, date_text):
    """æ¸…ç†æ¨™é¡Œ"""
    title = re.sub(r'20\d{2}[å¹´/.\-_]\d{1,2}[æœˆ/.\-_]\d{1,2}[æ—¥]?', '', title)
    title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\(PDF\)', '', title, flags=re.IGNORECASE)
    title = re.sub(r'[_\-]+', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    
    return title


if __name__ == "__main__":
    results = scrape()
    if results:
        print(f"\nğŸ“„ æ‰¾åˆ°çš„å ±å‘Š:")
        for i, r in enumerate(results, 1):
            print(f"\n{i}. [{r['Date']}] {r['Name'][:60]}")
            print(f"   ğŸ”— {r['Link']}")
