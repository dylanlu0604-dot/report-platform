from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, unquote
import time
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError
from scrapers.utils import is_within_30_days # å‡è¨­é€™æ˜¯ä½ åŸæœ¬çš„ utils

def scrape():
    """
    ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–çˆ¬èŸ² (Playwright å‹•æ…‹æ¸²æŸ“ç‰ˆ)
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– CTBC (ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–)...")
    reports = []
    seen_urls = set()
    
    base_url = "https://www.ctbcbank.com"
    target_url = "https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html"
    
    try:
        with sync_playwright() as p:
            # å•Ÿå‹•ç„¡é ­ç€è¦½å™¨
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()
            
            print("  ğŸŒ å•Ÿå‹• Playwright è¼‰å…¥å‹•æ…‹ç¶²é ...")
            # å‰å¾€ç›®æ¨™ç¶²é ï¼Œç­‰å¾…ç¶²è·¯é–’ç½® (ç¢ºä¿ JS è¼‰å…¥ä¸” API å›å‚³å®Œç•¢)
            page.goto(target_url, wait_until="networkidle", timeout=30000)
            
            # çµ¦äºˆé¡å¤–æ™‚é–“ç¢ºä¿ç•«é¢ä¸Šçš„å ±å‘Šåˆ—è¡¨å·²ç¶“æ¸²æŸ“
            try:
                page.wait_for_selector('a', timeout=10000)
                time.sleep(3) 
            except PlaywrightTimeoutError:
                print("  âš ï¸ ç­‰å¾…ç‰¹å®šå…ƒç´ è¶…æ™‚ï¼Œå˜—è©¦ç›´æ¥è§£æç•¶å‰é é¢...")
            
            # ç²å–æ¸²æŸ“å¾Œçš„å®Œæ•´ HTML
            html_content = page.content()
            soup = BeautifulSoup(html_content, 'html.parser')
            print(f"  ğŸ“„ æ¸²æŸ“å¾Œé é¢å¤§å°: {len(html_content):,} å­—å…ƒ")
            
            browser.close()

        # ==========================================
        # ç­–ç•¥ 1: å»£åŸŸ PDF æœå°‹ (æ²¿ç”¨ä½ åŸæœ¬çš„é‚è¼¯)
        # ==========================================
        print("  [ç­–ç•¥ 1] å»£åŸŸ PDF æœå°‹...")
        
        pdf_urls_raw = re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE)
        pdf_urls_raw.extend(re.findall(r'href=["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE))
        pdf_urls_raw.extend(re.findall(r'(https?://[^\s<>"\']+\.pdf)', html_content, re.IGNORECASE))
        
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if '.pdf' in href.lower():
                pdf_urls_raw.append(href)
        
        for elem in soup.find_all(attrs={'data-url': True}):
            url = elem.get('data-url', '')
            if '.pdf' in url.lower():
                pdf_urls_raw.append(url)
                
        for elem in soup.find_all(attrs={'data-href': True}):
            url = elem.get('data-href', '')
            if '.pdf' in url.lower():
                pdf_urls_raw.append(url)
        
        # å»é‡å’Œæ¸…ç†
        pdf_urls = []
        for url in set(pdf_urls_raw):
            url = url.strip().strip('"').strip("'")
            if url and '.pdf' in url.lower():
                full_url = urljoin(base_url, url)
                if full_url not in seen_urls:
                    pdf_urls.append(full_url)
                    seen_urls.add(full_url)
        
        print(f"    æ‰¾åˆ° {len(pdf_urls)} å€‹ PDF URL")
        
        for pdf_url in pdf_urls:
            title, date_text = extract_info_from_url(pdf_url)
            
            if not title or not date_text:
                link = soup.find('a', href=lambda x: x and pdf_url in urljoin(base_url, x))
                if link:
                    if not title:
                        title = extract_title_from_link(link)
                    if not date_text:
                        date_text = extract_date_from_link(link)
            
            if not title or len(title) < 5:
                filename = unquote(pdf_url.split('/')[-1].replace('.pdf', '').replace('.PDF', ''))
                title = filename
            
            if not date_text:
                continue
            if not is_within_30_days(date_text):
                continue
                
            title = clean_title(title, date_text)
            
            reports.append({
                "Source": "CTBC",
                "Date": date_text,
                "Name": title,
                "Link": pdf_url
            })
            
        # ==========================================
        # ç­–ç•¥ 2: å¦‚æœæ²’æ‰¾åˆ°,æœå°‹æ‰€æœ‰å¯èƒ½çš„å ±å‘Šé€£çµ
        # ==========================================
        if len(reports) == 0:
            print("  [ç­–ç•¥ 2] æœå°‹å ±å‘Šé€£çµ...")
            # (æ­¤è™•æ²¿ç”¨ä½ åŸæœ¬ç­–ç•¥ 2 èˆ‡ç­–ç•¥ 3 çš„ç¨‹å¼ç¢¼ï¼Œç›´æ¥è¤‡è£½è²¼ä¸Šå³å¯)
            # ...
            
    except Exception as e:
        print(f"  âŒ CTBC çˆ¬å–å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()

    print(f"  âœ… CTBC æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports

# åº•ä¸‹ä¿ç•™ä½ åŸæœ¬å¯«å¥½çš„ extract_info_from_url, extract_title_from_link, extract_date_from_link ç­‰ def
# ...
