import re
import time
from urllib.parse import urljoin, unquote
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError, Error as PlaywrightError
from scrapers.utils import HEADERS, is_within_30_days

def scrape():
    """
    ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–çˆ¬èŸ² (å¢å¼·ç‰ˆ)
    
    æ”¹é€²:
    1. å¢åŠ é‡è©¦æ©Ÿåˆ¶è™•ç†ç¶²è·¯éŒ¯èª¤
    2. é™ç´šç­–ç•¥: Playwrightå¤±æ•—æ™‚ä½¿ç”¨requests
    3. æ›´å¥½çš„éŒ¯èª¤è™•ç†
    """
    print("ğŸ” æ­£åœ¨çˆ¬å– CTBC (ä¸­åœ‹ä¿¡è¨—éŠ€è¡Œ - å¸‚å ´è©•è«–)...")
    reports = []
    
    base_url = "https://www.ctbcbank.com"
    target_url = "https://www.ctbcbank.com/twrbo/zh_tw/wm_index/wm_investreport/market-comment.html"
    
    # å˜—è©¦ä½¿ç”¨ Playwright (æœ€å¤šé‡è©¦ 3 æ¬¡)
    html_content = None
    for attempt in range(3):
        try:
            html_content = scrape_with_playwright(target_url, attempt + 1)
            if html_content:
                break
        except Exception as e:
            print(f"  âš ï¸ Playwright å˜—è©¦ {attempt + 1}/3 å¤±æ•—: {type(e).__name__}")
            if attempt < 2:
                time.sleep(5)  # ç­‰å¾… 5 ç§’å¾Œé‡è©¦
    
    # å¦‚æœ Playwright å®Œå…¨å¤±æ•—,é™ç´šä½¿ç”¨ requests
    if not html_content:
        print("  ğŸ”„ Playwright å¤±æ•—,é™ç´šä½¿ç”¨ requests...")
        try:
            import requests
            resp = requests.get(target_url, headers=HEADERS, timeout=30)
            html_content = resp.text
            print(f"  âœ“ requests æˆåŠŸç²å–é é¢ ({len(html_content):,} å­—å…ƒ)")
        except Exception as e:
            print(f"  âŒ requests ä¹Ÿå¤±æ•—: {e}")
            return reports
    
    # è§£æ HTML
    try:
        soup = BeautifulSoup(html_content, 'html.parser')
        seen_urls = set()
        
        # ç­–ç•¥ 1: å»£åŸŸ PDF æœå°‹
        print("  [ç­–ç•¥ 1] å»£åŸŸ PDF æœå°‹...")
        
        pdf_urls_raw = []
        
        # æ–¹æ³• A: æ­£å‰‡æœå°‹
        pdf_urls_raw.extend(re.findall(r'["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE))
        pdf_urls_raw.extend(re.findall(r'href=["\']([^"\']*\.pdf[^"\']*)["\']', html_content, re.IGNORECASE))
        pdf_urls_raw.extend(re.findall(r'(https?://[^\s<>"\']+\.pdf)', html_content, re.IGNORECASE))
        
        # CTBC å°ˆå±¬çš„ API æ ¼å¼
        pdf_urls_raw.extend(re.findall(r'(/IB/api/adapters/IB_Adapter/resource/report/[^"\']+)', html_content, re.IGNORECASE))
        
        # æ–¹æ³• B: BeautifulSoup
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')
            if '.pdf' in href.lower() or '/resource/report/' in href.lower():
                pdf_urls_raw.append(href)
        
        # æ–¹æ³• C: data å±¬æ€§
        for elem in soup.find_all(attrs={'data-url': True}):
            url = elem.get('data-url', '')
            if '.pdf' in url.lower() or '/resource/report/' in url.lower():
                pdf_urls_raw.append(url)
        
        for elem in soup.find_all(attrs={'data-href': True}):
            url = elem.get('data-href', '')
            if '.pdf' in url.lower() or '/resource/report/' in url.lower():
                pdf_urls_raw.append(url)
        
        # å»é‡å’Œæ¸…ç†
        target_urls = []
        for url in set(pdf_urls_raw):
            url = url.strip().strip('"').strip("'")
            if url and ('.pdf' in url.lower() or '/resource/report/' in url.lower()):
                full_url = urljoin(base_url, url)
                if full_url not in seen_urls:
                    target_urls.append(full_url)
                    seen_urls.add(full_url)
        
        print(f"    æ‰¾åˆ° {len(target_urls)} å€‹ç›®æ¨™ URL")
        
        # è™•ç†æ¯å€‹ URL
        for target_url_item in target_urls:
            title, date_text = extract_info_from_url(target_url_item)
            
            # å¦‚æœ URL æ²’è³‡è¨Š,å˜—è©¦å¾ HTML æå–
            if not title or not date_text:
                link = soup.find('a', href=lambda x: x and target_url_item in urljoin(base_url, x))
                
                if not link:
                    report_id = target_url_item.split('/')[-1]
                    link = soup.find(lambda tag: tag.name in ['a', 'div', 'li', 'td'] and report_id in str(tag))
                
                if link:
                    if not title:
                        title = extract_title_from_link(link)
                    if not date_text:
                        date_text = extract_date_from_link(link)
            
            # å¦‚æœé‚„æ˜¯æ²’æ¨™é¡Œ,ç”¨æª”å
            if not title or len(title) < 5:
                filename = unquote(target_url_item.split('/')[-1].replace('.pdf', '').replace('.PDF', ''))
                title = filename
            
            # æ²’æ—¥æœŸå°±è·³é
            if not date_text:
                continue
            
            # æª¢æŸ¥ 30 å¤©å…§
            if not is_within_30_days(date_text):
                continue
            
            # æ¸…ç†æ¨™é¡Œ
            title = clean_title(title, date_text)
            
            reports.append({
                "Source": "CTBC",
                "Date": date_text,
                "Name": title,
                "Link": target_url_item
            })
        
        # ç­–ç•¥ 2: å ±å‘Šé€£çµæœå°‹
        if len(reports) == 0:
            print("  [ç­–ç•¥ 2] æœå°‹å ±å‘Šé€£çµ...")
            
            keywords = [
                'å ±å‘Š', 'è©•è«–', 'åˆ†æ', 'å¸‚å ´', 'å±•æœ›', 'è§€é»',
                'report', 'analysis', 'market', 'comment', 'review'
            ]
            
            report_count = 0
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                text = link.get_text(strip=True)
                
                if any(exclude in href.lower() for exclude in ['javascript:', 'mailto:', '#']):
                    continue
                
                if any(kw in text or kw in href for kw in keywords):
                    report_count += 1
            
            print(f"    æ‰¾åˆ° {report_count} å€‹å¯èƒ½çš„å ±å‘Šé€£çµ")
    
    except Exception as e:
        print(f"  âŒ HTML è§£æå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
    
    print(f"  âœ… CTBC æ‰¾åˆ° {len(reports)} ç­†å ±å‘Š")
    return reports


def scrape_with_playwright(url, attempt):
    """
    ä½¿ç”¨ Playwright æŠ“å–é é¢
    
    Args:
        url: ç›®æ¨™ URL
        attempt: ç•¶å‰æ˜¯ç¬¬å¹¾æ¬¡å˜—è©¦
    
    Returns:
        HTML å…§å®¹æˆ– None
    """
    print(f"  ğŸŒ Playwright å˜—è©¦ {attempt}/3...")
    
    try:
        with sync_playwright() as p:
            # é…ç½®ç€è¦½å™¨
            browser = p.chromium.launch(
                headless=True,
                args=[
                    '--disable-dev-shm-usage',  # é¿å…å…±äº«è¨˜æ†¶é«”å•é¡Œ
                    '--no-sandbox',             # GitHub Actions éœ€è¦
                    '--disable-setuid-sandbox'
                ]
            )
            
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                viewport={'width': 1920, 'height': 1080}
            )
            
            page = context.new_page()
            
            # è¨­ç½®è¼ƒçŸ­çš„è¶…æ™‚,å¤±æ•—å°±å¿«é€Ÿé‡è©¦
            try:
                page.goto(url, wait_until="domcontentloaded", timeout=30000)
                print(f"    âœ“ é é¢è¼‰å…¥æˆåŠŸ")
            except PlaywrightTimeoutError:
                print(f"    âš ï¸ è¼‰å…¥è¶…æ™‚,ä½†ç¹¼çºŒå˜—è©¦...")
            except PlaywrightError as e:
                if "ERR_NETWORK_CHANGED" in str(e):
                    print(f"    âš ï¸ ç¶²è·¯åˆ‡æ›éŒ¯èª¤,å°‡é‡è©¦...")
                    browser.close()
                    return None
                raise
            
            # ç­‰å¾…å‹•æ…‹å…§å®¹
            print(f"    â³ ç­‰å¾…å‹•æ…‹å…§å®¹ (5ç§’)...")
            time.sleep(5)
            
            # ç²å–å…§å®¹
            html_content = page.content()
            print(f"    âœ“ ç²å–é é¢å…§å®¹ ({len(html_content):,} å­—å…ƒ)")
            
            browser.close()
            return html_content
            
    except Exception as e:
        print(f"    âœ— å¤±æ•—: {type(e).__name__}")
        return None


def extract_info_from_url(url):
    """å¾ URL æå–æ¨™é¡Œå’Œæ—¥æœŸ"""
    title = None
    date_text = None
    
    url_decoded = unquote(url)
    filename = url_decoded.split('/')[-1].replace('.pdf', '').replace('.PDF', '')
    
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}[_-]\d{1,2}[_-]\d{1,2}',
        r'20\d{2}\d{2}\d{2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, filename)
        if match:
            date_text = match.group(0)
            if len(date_text) == 8 and date_text.isdigit():
                date_text = f"{date_text[:4]}/{date_text[4:6]}/{date_text[6:8]}"
            break
    
    if date_text:
        title = re.sub(r'20\d{2}[å¹´_\-/]\d{1,2}[æœˆ_\-/]\d{1,2}[æ—¥]?', '', filename)
        title = re.sub(r'20\d{2}\d{2}\d{2}', '', title)
        
        # å¦‚æœåªå‰©æµæ°´è™Ÿ,è¨­ç‚º None
        if re.match(r'^[-_A-Za-z0-9]+$', title.strip()):
            title = None
    else:
        title = filename
    
    return title, date_text


def extract_title_from_link(link):
    """å¾é€£çµå…ƒç´ æå–æ¨™é¡Œ"""
    title = link.get_text(strip=True)
    
    if not title or len(title) < 5:
        title = link.get('title', '')
    
    if not title or len(title) < 5:
        parent = link.find_parent(['li', 'div', 'td'])
        if parent:
            title = re.sub(r'\s+', ' ', parent.get_text(strip=True))
    
    return title


def extract_date_from_link(link):
    """å¾é€£çµå‘¨åœæå–æ—¥æœŸ"""
    parent = link.find_parent(['li', 'div', 'tr', 'td', 'article'])
    if parent:
        search_text = parent.get_text()
    else:
        search_text = link.get_text()
    
    return extract_date_from_text(search_text)


def extract_date_from_text(text):
    """å¾æ–‡å­—ä¸­æå–æ—¥æœŸ"""
    date_patterns = [
        r'20\d{2}å¹´\d{1,2}æœˆ\d{1,2}æ—¥',
        r'20\d{2}/\d{1,2}/\d{1,2}',
        r'20\d{2}\.\d{1,2}\.\d{1,2}',
        r'20\d{2}-\d{1,2}-\d{1,2}',
    ]
    
    for pattern in date_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(0)
    
    return None


def clean_title(title, date_text):
    """æ¸…ç†æ¨™é¡Œ"""
    title = re.sub(r'20\d{2}[å¹´/.\-_]\d{1,2}[æœˆ/.\-_]\d{1,2}[æ—¥]?', '', title)
    title = re.sub(r'\.pdf$', '', title, flags=re.IGNORECASE)
    title = re.sub(r'\(PDF\)', '', title, flags=re.IGNORECASE)
    title = re.sub(r'[_\-]+', ' ', title)
    title = re.sub(r'\s+', ' ', title).strip()
    
    return title


if __name__ == "__main__":
    results = scrape()
    if results:
        print(f"\nğŸ“„ æ‰¾åˆ°çš„å ±å‘Š:")
        for i, r in enumerate(results, 1):
            print(f"\n{i}. [{r['Date']}] {r['Name'][:60]}")
            print(f"   ğŸ”— {r['Link']}")
